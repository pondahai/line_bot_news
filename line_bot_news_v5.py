# ==============================================================================
# line_bot_v5.py
# æœ€çµ‚æ•´åˆç‰ˆæœ¬
#
# åŠŸèƒ½äº®é»:
# - è¢«å‹•ç›£è½ç¾¤çµ„å°è©±ï¼Œé€é /bot æŒ‡ä»¤è§¸ç™¼ï¼Œå¯¦ç¾ä¸Šä¸‹æ–‡ç†è§£ã€‚
# - æ”¯æ´ç¾¤çµ„å…§ç”¨æˆ¶åç¨±ç²å–èˆ‡å¿«å–ã€‚
# - æ•´åˆäº†ã€Œä¸€æ¬¡æ€§æ–°èæŸ¥è©¢ã€èˆ‡ã€ŒæŒä¹…åŒ–è¨‚é–±ç®¡ç†ã€çš„å®Œæ•´æŒ‡ä»¤ç³»çµ±ã€‚
# - æ‰€æœ‰è€—æ™‚ä»»å‹™ (æ–°èè™•ç†) å‡åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­è™•ç†ã€‚
# - å®šæ™‚æ¨æ’­ä»»å‹™æ¡ç”¨ã€Œä»»å‹™éˆã€æ¨¡å¼åºåˆ—åŒ–åŸ·è¡Œï¼Œä¿è­·å¾Œç«¯ LLM ä¼ºæœå™¨ã€‚
# - Selenium Driver èƒ½å¤ æ™ºæ…§åˆ¤æ–·ä½œæ¥­ç³»çµ±ï¼Œé©æ‡‰ä¸åŒéƒ¨ç½²ç’°å¢ƒã€‚
# ==============================================================================

# --- Python Standard Libraries ---
import os
import platform
import sys
import time
import json
import logging
import hashlib
import hmac
import base64
import re
import atexit
import argparse
from datetime import datetime, timedelta
import urllib.parse

# --- Third-party Libraries ---
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from apscheduler.schedulers.background import BackgroundScheduler
import feedparser

# --- Newspaper3k for article scraping ---
from newspaper import Article, Config

# --- Selenium for dynamic content scraping ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ==============================================================================
# --- ç’°å¢ƒè¨­å®šã€æ—¥èªŒèˆ‡ Flask åˆå§‹åŒ– ---
# ==============================================================================
load_dotenv()
app = Flask(__name__)

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]',
    stream=sys.stdout
)

# --- å…¨åŸŸè®Šæ•¸èˆ‡å¸¸æ•¸ ---
BOT_TRIGGER_WORD = os.getenv("BOT_TRIGGER_WORD", "/bot")
OPENAI_COMPLETION_MODEL = os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com")
TARGET_USER_ID_FOR_TESTING = os.getenv("TARGET_USER_ID_FOR_TESTING")

VISUAL_SEPARATION_DELAY = float(os.getenv("VISUAL_SEPARATION_DELAY", "1.0"))
DEFAULT_NEWS_KEYWORDS = "å¤§å‹èªè¨€æ¨¡å‹ OR LLM OR ç”Ÿæˆå¼AI OR OpenAI OR Gemini OR Claude"
USER_PREFERENCES_FILE = "user_preferences.json"
CONVERSATION_HISTORY_FILE = "conversation_history.json"
MAX_HISTORY_MESSAGES = int(os.getenv("MAX_HISTORY_MESSAGES", "50"))
NEWS_FETCH_TARGET_COUNT = 7
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'

NEWS_CACHE_FILE = "news_cache.json"
NEWS_CACHE_DURATION_SECONDS = 3600  # 1 å°æ™‚

# --- ç”¨æˆ¶å€‹äººè³‡æ–™å¿«å– (in-memory) ---
USER_PROFILE_CACHE = {}
CACHE_EXPIRATION_SECONDS = 3600  # å¿«å– 1 å°æ™‚

MAX_MESSAGE_LENGTH = int(os.getenv("MAX_MESSAGE_LENGTH", "4800"))

# --- å…©éšæ®µæ‘˜è¦çš„ LLM Prompt è¨­å®š ---
PROMPT_FOR_INDIVIDUAL_SUMMARY = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æ–°èç·¨è¼¯ï¼Œå°ˆé•·æ˜¯å¿«é€Ÿæç…‰æ–‡ç« æ ¸å¿ƒã€‚è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–°èå…§æ–‡ï¼Œæ¿ƒç¸®æˆä¸€æ®µä¸è¶…é150å­—çš„å®¢è§€ã€ç²¾ç°¡ä¸­æ–‡æ‘˜è¦ã€‚"
    "æ‘˜è¦æ‡‰åŒ…å«æœ€é—œéµçš„äººç‰©ã€äº‹ä»¶ã€æ•¸æ“šå’Œçµè«–ã€‚è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹é ­æˆ–çµå°¾çš„å®¢å¥—è©±ã€‚"
)
PROMPT_FOR_FINAL_AGGREGATION = (
    "ä½ æ˜¯ä¸€ä½é¢¨è¶£å¹½é»˜ã€çŸ¥è­˜æ·µåšçš„ç§‘æŠ€æ–°è Podcast ä¸»æŒäººã€‚ä½ çš„è½çœ¾æ˜¯ Line ç”¨æˆ¶ï¼Œä»–å€‘å–œæ­¡è¼•é¬†ã€æ˜“æ‡‚ä¸”å¸¶æœ‰ Emoji çš„å…§å®¹ã€‚"
    "æ¥ä¸‹ä¾†æˆ‘æœƒæä¾›æ•¸å‰‡ã€Œå·²ç¶“è¢«ç²¾ç°¡éçš„æ–°èæ‘˜è¦ã€ã€‚è«‹ä½ æ ¹æ“šé€™äº›æ‘˜è¦ï¼Œç™¼æ®ä½ çš„ä¸»æŒé¢¨æ ¼ï¼Œå°‡å®ƒå€‘æ•´åˆæˆä¸€ç¯‡é€£è²«çš„è«‡è©±æ€§å…§å®¹ã€‚"
    "ä½ çš„ä»»å‹™æ˜¯ï¼š\n"
    "1. ç”¨ç”Ÿå‹•çš„èªæ°£é–‹å ´ï¼Œå¸å¼•è½çœ¾æ³¨æ„ã€‚\n"
    "2. å°‡å„å‰‡æ–°èæ‘˜è¦è‡ªç„¶åœ°ä¸²é€£èµ·ä¾†ï¼Œå¯ä»¥åŠ ä¸Šä½ çš„è©•è«–æˆ–è§€é»ä¾†éŠœæ¥ï¼Œä½†ä¸è¦æœæ’°ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚\n"
    "3. åœ¨æåˆ°æ¯å‰‡æ–°èçš„é‡é»å¾Œï¼Œè«‹å‹™å¿…é™„ä¸Šé€™å‰‡æ–°èçš„åŸå§‹æ¨™é¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
    "   - æ¨™é¡Œï¼š[åŸå§‹æ–°èæ¨™é¡Œ]\n"
    "4. å…¨ç¨‹å¤šä½¿ç”¨ Emoji ä¾†å¢åŠ æ´»æ½‘æ„Ÿã€‚\n"
    "5. è¦åš´è‚…æ‡‰å°æ¯å‰‡æ–°èçš„è² é¢æƒ…ç·’ã€‚\n"
    "6. æœ€å¾Œçµè«–è¦åŠ è¨»é€™æ˜¯AIç”Ÿæˆçš„å…§å®¹ï¼Œè®€è€…æ‡‰æ³¨æ„æ­£ç¢ºæ€§ã€‚\n"
    "7. ç¸½çµçš„å›ç­”å­—æ•¸é™åˆ¶åœ¨500å­—ä»¥ä¸‹ä»¥ç¬¦åˆé€šè¨Šè»Ÿé«”çš„é™åˆ¶ã€‚\n"
)

# --- æ©Ÿå™¨äººæŒ‡ä»¤å¹«åŠ©è¨Šæ¯ ---
HELP_MESSAGE = """
å“ˆå›‰ï¼ğŸ‘‹ æˆ‘æ˜¯ä½ çš„ AI åŠ©ç†ï¼

ä½ å¯ä»¥é€é `/bot` æŒ‡ä»¤èˆ‡æˆ‘äº’å‹•ã€‚

ğŸ“°ã€æ–°èåŠŸèƒ½ã€‘
ğŸ”¹ `/bot æ–°è`
   ç«‹å³å–å¾—ä¸€ç¯‡ AI ä¸»é¡Œçš„æ–°èæ‘˜è¦ã€‚
ğŸ”¹ `/bot æ–°è é—œéµå­—:ä½ æƒ³çœ‹çš„å…§å®¹`
   ç«‹å³æŸ¥è©¢ç‰¹å®šä¸»é¡Œçš„æ–°èã€‚
ğŸ”¹ `/bot è¨‚é–±`
   è¨‚é–±æ¯æ—¥ AI æ–°èæ¨æ’­ã€‚
ğŸ”¹ `/bot è¨‚é–± [ä½ çš„ä¸»é¡Œ]`
   è¨‚é–±æ¯æ—¥ç‰¹å®šä¸»é¡Œçš„æ–°èã€‚
ğŸ”¹ `/bot æŸ¥çœ‹è¨‚é–±`
   çœ‹çœ‹ä½ ç›®å‰è¨‚é–±äº†ä»€éº¼ã€‚
ğŸ”¹ `/bot å–æ¶ˆè¨‚é–±`
   å–æ¶ˆæ¯æ—¥æ–°èæ¨æ’­ã€‚

ğŸ’¬ã€éš¨æ„èŠå¤©ã€‘
é™¤äº†æ–°èï¼Œä¹Ÿå¯ä»¥éš¨æ™‚ç”¨ `/bot` å•æˆ‘ä»»ä½•å•é¡Œå–”ï¼
ç¯„ä¾‹ï¼š`/bot å¹«æˆ‘è¦åŠƒä¸€ä¸‹é€±æœ«è¡Œç¨‹`
"""

# ==============================================================================
# --- æ–°èæ“·å–æ¨¡çµ„ ---
# ==============================================================================
newspaper_config = Config()
newspaper_config.browser_user_agent = USER_AGENT
newspaper_config.request_timeout = 15
newspaper_config.memoize_articles = False

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1024x400")
chrome_options.add_argument(f"user-agent={USER_AGENT}")

def get_real_url(google_news_url):
    try:
        response = requests.head(google_news_url, allow_redirects=True, timeout=15)
        return response.url
    except requests.RequestException as e:
        logging.warning(f"    [éŒ¯èª¤] ç„¡æ³•è§£æè·³è½‰é€£çµ {google_news_url}: {e}")
        return None

def fetch_article_with_selenium(url):
    logging.info(f"    [å‚™æ´] å˜—è©¦ä½¿ç”¨ Selenium æŠ“å–å‹•æ…‹å…§å®¹: {url[:70]}...")
    driver = None
    try:
        current_os = platform.system()
        logging.info(f"    [Selenium è¨­å®š] åµæ¸¬åˆ°ç›®å‰ä½œæ¥­ç³»çµ±ç‚º: {current_os}")

        if current_os == "Linux":
            chromedriver_path = "/usr/bin/chromedriver"
            logging.info(f"    [Selenium è¨­å®š] Linux ç³»çµ±ï¼Œä½¿ç”¨æ‰‹å‹•æŒ‡å®šçš„è·¯å¾‘: {chromedriver_path}")
            if not os.path.exists(chromedriver_path):
                logging.error(f"    [Selenium åš´é‡éŒ¯èª¤] åœ¨ Linux ç³»çµ±ä¸Šæ‰¾ä¸åˆ°æŒ‡å®šçš„ chromedriver: {chromedriver_path}")
                logging.error("    è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ `sudo apt-get install chromium-chromedriver`ï¼Œæˆ–è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
                return None
            service = Service(executable_path=chromedriver_path)
        else:
            logging.info("    [Selenium è¨­å®š] é Linux ç³»çµ±ï¼Œä½¿ç”¨ webdriver-manager è‡ªå‹•ä¸‹è¼‰/ç®¡ç† driverã€‚")
            service = Service(ChromeDriverManager().install())
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        time.sleep(4)
        return driver.page_source
    except Exception as e:
        logging.error(f"    [Selenium éŒ¯èª¤] æŠ“å– {url[:70]}... å¤±æ•—: {e}", exc_info=True)
        return None
    finally:
        if driver:
            driver.quit()

def fetch_and_parse_articles(custom_query=None, limit=NEWS_FETCH_TARGET_COUNT):
    query_to_use = custom_query.strip() if custom_query and custom_query.strip() else DEFAULT_NEWS_KEYWORDS
    encoded_query = urllib.parse.quote_plus(query_to_use)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    logging.info(f">>> é–‹å§‹å¾ Google News RSS å–å¾—æ–°èåˆ—è¡¨ (é—œéµå­—: '{query_to_use}')")
    feed = feedparser.parse(rss_url)

    if feed.bozo:
        logging.error(f"ç„¡æ³•è§£æ RSS feedã€‚éŒ¯èª¤è³‡è¨Š: {feed.bozo_exception}")
        return []

    successful_articles = []
    processed_urls = set()
    
    logging.info(f">>> æ‰¾åˆ° {len(feed.entries)} å‰‡æ–°èï¼Œé–‹å§‹é€ä¸€çˆ¬å–å…§æ–‡ï¼Œç›®æ¨™ {limit} å‰‡...")

    for entry in feed.entries:
        if len(successful_articles) >= limit: break
        logging.info(f"--- æ­£åœ¨è™•ç†: {entry.title}")
        real_url = get_real_url(entry.link)
        if not real_url or real_url in processed_urls:
            logging.warning("    [è·³é] ç„¡æ³•å–å¾—çœŸå¯¦ URL æˆ– URL é‡è¤‡ã€‚")
            continue
        processed_urls.add(real_url)
        try:
            article = Article(real_url, language='zh', config=newspaper_config)
            article.download()
            article.parse()
            if len(article.text) < 200:
                logging.warning("    [è­¦å‘Š] æ¨™æº–æ–¹æ³•æŠ“å–å…§å®¹éçŸ­ï¼Œå•Ÿç”¨ Selenium å‚™æ´ã€‚")
                html_content = fetch_article_with_selenium(real_url)
                if html_content:
                    article.download(input_html=html_content)
                    article.parse()
            if article.title and len(article.text) > 200:
                logging.info(f"    [æˆåŠŸ] å·²å–å¾—æ–‡ç« : {article.title}")
                successful_articles.append({'title': article.title, 'text': article.text, 'url': real_url, 'source': entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else "æœªçŸ¥ä¾†æº"})
            else:
                logging.warning(f"    [å¤±æ•—] ä½¿ç”¨æ‰€æœ‰æ–¹æ³•å¾Œï¼Œä»ç„¡æ³•è§£æå‡ºè¶³å¤ å…§æ–‡ã€‚URL: {real_url}")
        except Exception as e:
            logging.error(f"    [å¤±æ•—] è™•ç†æ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ã€‚ URL: {real_url}, åŸå› : {e}", exc_info=True)
        finally:
            time.sleep(1)
    logging.info(f">>> æ–°èå…§æ–‡æ“·å–å®Œæˆï¼Œå…±æˆåŠŸå–å¾— {len(successful_articles)} ç¯‡ã€‚")
    return successful_articles

# ==============================================================================
# --- OpenAI & LLM äº’å‹•æ¨¡çµ„ ---
# ==============================================================================
def call_openai_api(messages, model=OPENAI_COMPLETION_MODEL, max_tokens=4000, temperature=0.7):
    if not OPENAI_API_KEY:
        logging.error("OPENAI_API_KEY is not set.")
        return "æŠ±æ­‰ï¼ŒAPI Key æœªè¨­å®šï¼Œç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json", "ngrok-skip-browser-warning": "true"}
    data = {"model": model, "messages": messages, "max_tokens": max_tokens, "temperature": temperature}
    try:
        response = requests.post(f"{OPENAI_BASE_URL}/v1/chat/completions", headers=headers, json=data, timeout=980)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"].strip()
        logging.info(f"OpenAI API å‘¼å«æˆåŠŸï¼Œæ¨¡å‹: {model}ï¼Œå›æ‡‰é•·åº¦: {len(content)}")
        return content
    except requests.exceptions.Timeout:
        logging.error(f"OpenAI API request timed out. Model: {model}")
        return f"æŠ±æ­‰ï¼Œè«‹æ±‚ OpenAI ({model}) æœå‹™è¶…æ™‚ã€‚"
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI API request error: {e}. Model: {model}")
        return f"æŠ±æ­‰ï¼Œé€£æ¥ OpenAI ({model}) æœå‹™æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    except (KeyError, IndexError, TypeError) as e:
        response_text = response.text if 'response' in locals() else 'N/A'
        logging.error(f"OpenAI API response format error: {e} - Response: {response_text}")
        return f"æŠ±æ­‰ï¼ŒOpenAI ({model}) å›æ‡‰æ ¼å¼æœ‰å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unexpected error in call_openai_api: {e}", exc_info=True)
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›è¦†æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ã€‚"

def generate_chat_response(context_id, prompt_text):
    system_prompt = (
        "ä½ æ˜¯ä¸€å€‹åœ¨ Line ç¾¤çµ„æˆ–ç§èŠä¸­çš„èŠå¤©æ©Ÿå™¨äººã€‚ä½ çš„å›ç­”è¦ç²¾ç°¡ã€å£èªåŒ–ï¼Œä½¿ç”¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡ã€‚"
        "ä½ æœƒæ”¶åˆ°ä¸€æ®µåŒ…å«å¤šäººå°è©±çš„æ­·å²ç´€éŒ„ï¼Œæ¯å¥è©±å‰é¢å¯èƒ½æœƒæ¨™ç¤ºç™¼è¨€è€…ã€‚è«‹æ ¹æ“šå®Œæ•´çš„ä¸Šä¸‹æ–‡é€²è¡Œå›ç­”ã€‚"
        "å¦‚æœç­”æ¡ˆéœ€è¦æ€è€ƒæ­¥é©Ÿï¼Œè«‹å°‡æ€è€ƒéç¨‹ç”¨ <think> å’Œ </think> æ¨™ç±¤åŒ…èµ·ä¾†ã€‚"
    )
    context_history = CONVERSATION_HISTORY.get(context_id, [])
    messages_for_api = [{"role": "system", "content": system_prompt}] + context_history
    bot_response = call_openai_api(messages_for_api)
    return bot_response

# ==============================================================================
# --- æ–°èæ‘˜è¦èˆ‡æ•´åˆæ¨¡çµ„ ---
# ==============================================================================
def summarize_news_flow(articles_data):
    if not articles_data: return "ä»Šå¤©æ²’æœ‰æŠ“å–åˆ°ç›¸é—œæ–°èå¯ä¾›æ‘˜è¦ã€‚"
    logging.info("--- é–‹å§‹ç¬¬ä¸€éšæ®µæ‘˜è¦ï¼šé€ç¯‡ç²¾ç°¡ ---")
    individual_summaries = []
    for i, article in enumerate(articles_data):
        logging.info(f"  æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(articles_data)} ç¯‡: {article['title']}")
        content_to_summarize = article['text'][:8000]
        user_prompt = f"æ–°èæ¨™é¡Œï¼š{article['title']}\n\næ–°èå…§æ–‡ï¼š\n{content_to_summarize}"
        raw_summary = call_openai_api([{"role": "system", "content": PROMPT_FOR_INDIVIDUAL_SUMMARY}, {"role": "user", "content": user_prompt}], model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini"), max_tokens=500, temperature=0.2)
        if raw_summary.startswith("æŠ±æ­‰ï¼Œ"):
            logging.warning(f"  [è·³é] ç¬¬ {i+1} ç¯‡æ–°èæ‘˜è¦å¤±æ•—: {raw_summary}")
            continue
        think_pattern = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)
        cleaned_summary = re.sub(think_pattern, '', raw_summary).strip()
        if len(raw_summary) != len(cleaned_summary): logging.info(f"  å·²æ¸…ç†æ‰ <think> æ¨™ç±¤ã€‚")
        individual_summaries.append({'title': article['title'], 'url': article['url'], 'summary': cleaned_summary})
        logging.info(f"  æ‘˜è¦å®Œæˆï¼Œé•·åº¦: {len(cleaned_summary)} å­—")
        logging.info(f"  ç­‰å¾…30ç§’ é¿å…LLMé€Ÿç‡é™åˆ¶")
        time.sleep(30) # é™ä½LLMé€Ÿç‡
    if not individual_summaries: return "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆéç¨‹ç™¼ç”Ÿå•é¡Œï¼Œç„¡æ³•ç”¢å‡ºå…§å®¹ã€‚"
    logging.info("--- é–‹å§‹ç¬¬äºŒéšæ®µæ‘˜è¦ï¼šå½™æ•´ç”Ÿæˆ Podcast å…§å®¹ ---")
    summaries_for_prompt = [f"æ–°è {i+1}:\næ¨™é¡Œ: {item['title']}\næ‘˜è¦å…§å®¹: {item['summary']}\n---" for i, item in enumerate(individual_summaries)]
    final_user_prompt = "\n".join(summaries_for_prompt)
    final_summary = call_openai_api([{"role": "system", "content": PROMPT_FOR_FINAL_AGGREGATION}, {"role": "user", "content": final_user_prompt}], model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o"), max_tokens=3000, temperature=0.7)
    return final_summary

# ==============================================================================
# --- Line Bot åŸºç¤åŠŸèƒ½èˆ‡è³‡æ–™è™•ç† ---
# ==============================================================================
def load_json_data(file_path):
    try:
        with open(file_path, "r", encoding='utf-8') as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return {}

def save_json_data(data, file_path):
    try:
        with open(file_path, "w", encoding='utf-8') as f: json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e: logging.error(f"å„²å­˜æª”æ¡ˆ {file_path} å¤±æ•—: {e}")

USER_PREFERENCES = load_json_data(USER_PREFERENCES_FILE)
CONVERSATION_HISTORY = load_json_data(CONVERSATION_HISTORY_FILE)
NEWS_CACHE = load_json_data(NEWS_CACHE_FILE) # <-- æ–°å¢é€™ä¸€è¡Œ

def validate_signature(request_body_bytes, signature_header):
    if not LINE_CHANNEL_SECRET: return True
    hash_obj = hmac.new(LINE_CHANNEL_SECRET.encode('utf-8'), request_body_bytes, hashlib.sha256)
    generated_signature = base64.b64encode(hash_obj.digest()).decode('utf-8')
    return hmac.compare_digest(generated_signature, signature_header)

def split_long_message(text, limit=None):
    """
    å°‡é•·è¨Šæ¯åˆ†å‰²æˆå¤šå€‹ç¬¦åˆ Line é•·åº¦é™åˆ¶çš„çŸ­è¨Šæ¯ã€‚
    """
    # *** ä¿®æ”¹æ ¸å¿ƒ ***
    # å¦‚æœå¤–éƒ¨æ²’æœ‰å‚³å…¥ limitï¼Œå‰‡åœ¨å‡½å¼å…§éƒ¨ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    if limit is None:
        limit = MAX_MESSAGE_LENGTH
    # *** ä¿®æ”¹çµæŸ ***

    if not text or not text.strip(): return []
    if len(text) <= limit: return [text.strip()]
    
    messages = []
    chunks = []
    current_chunk = ""
    paragraphs = text.split('\n')
    
    for para in paragraphs:
        if len(current_chunk) + len(para) + 1 <= limit:
            current_chunk += para + '\n'
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            # å¦‚æœå–®ä¸€æ®µè½å°±è¶…éé•·åº¦ï¼Œå¼·åˆ¶åˆ†å‰²
            if len(para) > limit:
                for i in range(0, len(para), limit):
                    chunks.append(para[i:i+limit])
            else:
                current_chunk = para + '\n'
    
    if current_chunk:
        chunks.append(current_chunk.strip())

    if len(chunks) > 1:
        for i, chunk in enumerate(chunks):
            messages.append(f"({i+1}/{len(chunks)})\n{chunk}")
    else:
        messages = chunks
        
    return [m for m in messages if m]

def send_line_messages(context_id, reply_token_or_none, text_messages_list):
    """
    *** å·²ä¿®æ­£ v3ï¼šè² è²¬è™•ç† reply/push åˆ‡æ›å’Œå»¶é² ***
    """
    if not text_messages_list: return

    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}", "Content-Type": "application/json"}
    
    # å˜—è©¦ç”¨ reply token ç™¼é€ç¬¬ä¸€å‰‡è¨Šæ¯
    is_first_message_replied = False
    if reply_token_or_none:
        payload = {"replyToken": reply_token_or_none, "messages": [{"type": "text", "text": str(text_messages_list[0])}]}
        try:
            response = requests.post("https://api.line.me/v2/bot/message/reply", headers=headers, json=payload, timeout=20)
            response.raise_for_status()
            logging.info(f"Reply message sent to {context_id}.")
            is_first_message_replied = True
        except requests.exceptions.RequestException as e:
            logging.error(f"Reply API failed (will fallback to Push API): {e}")
            # å³ä½¿ reply å¤±æ•—ï¼Œæˆ‘å€‘ä»ç„¶ç¹¼çºŒå˜—è©¦ç”¨ push ç™¼é€æ‰€æœ‰è¨Šæ¯
    
    # ä½¿ç”¨ Push API ç™¼é€å‰©é¤˜çš„è¨Šæ¯ï¼ˆæˆ–æ‰€æœ‰è¨Šæ¯ï¼Œå¦‚æœ reply å¤±æ•—ï¼‰
    start_index = 1 if is_first_message_replied else 0
    for i in range(start_index, len(text_messages_list)):
        # åœ¨æ¯æ¬¡ push ä¹‹é–“éƒ½åŠ å…¥å»¶é²ï¼Œé€™æ˜¯æ¶ˆé™¤ 429 éŒ¯èª¤çš„é—œéµ
        if i > 0:
            time.sleep(10) # ç¨å¾®å¢åŠ å»¶é²æ™‚é–“
            
        payload = {"to": context_id, "messages": [{"type": "text", "text": str(text_messages_list[i])}]}
        try:
            response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
            # æˆ‘å€‘ä¸å†å° 429 åšè¤‡é›œçš„é‡è©¦ï¼Œè€Œæ˜¯å¾æºé ­ç”¨å»¶é²ä¾†é¿å…å®ƒ
            response.raise_for_status()
            logging.info(f"Push message (part {i+1}) sent to {context_id}.")
        except requests.exceptions.RequestException as e:
            logging.error(f"Push API failed for message part {i+1} to {context_id}: {e}")
            # å¦‚æœä¸€å‰‡ push å¤±æ•—ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡ä¸­æ­¢å¾ŒçºŒçš„ç™¼é€
            break
        
def get_user_profile(context_id, user_id):
    cache_key = (context_id, user_id)
    current_time = time.time()
    if cache_key in USER_PROFILE_CACHE and current_time - USER_PROFILE_CACHE[cache_key]['timestamp'] < CACHE_EXPIRATION_SECONDS:
        return USER_PROFILE_CACHE[cache_key]['displayName']
    if context_id.startswith('G') or context_id.startswith('R'): url = f"https://api.line.me/v2/bot/group/{context_id}/member/{user_id}"
    elif context_id.startswith('U'): url = f"https://api.line.me/v2/bot/profile/{user_id}"
    else: return "æœªçŸ¥ç”¨æˆ¶"
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        profile_data = response.json()
        display_name = profile_data.get("displayName", "ç„¡åæ°")
        USER_PROFILE_CACHE[cache_key] = {"displayName": display_name, "timestamp": current_time}
        logging.info(f"é€é API å–å¾—ç”¨æˆ¶ {user_id} çš„åç¨±: {display_name}ï¼Œä¸¦å·²æ›´æ–°å¿«å–ã€‚")
        return display_name
    except requests.exceptions.RequestException as e:
        logging.warning(f"ç„¡æ³•ç²å–ç”¨æˆ¶ {user_id} çš„å€‹äººè³‡æ–™: {e}")
        return "æŸä½æˆå“¡"

# ==============================================================================
# --- æ ¸å¿ƒæ¥­å‹™é‚è¼¯èˆ‡ Webhook äº‹ä»¶è™•ç† ---
# ==============================================================================
def generate_and_push_news_for_user(user_id, user_custom_keywords=None, is_immediate_push=False, reply_token=None):
    """
    *** å·²å‡ç´š v2ï¼šå¢åŠ äº†æ–°èæ‘˜è¦å¿«å–åŠŸèƒ½ ***
    ç‚ºæŒ‡å®šç”¨æˆ¶ç²å–ã€æ‘˜è¦ä¸¦æ¨æ’­æ–°èçš„å®Œæ•´æµç¨‹ã€‚
    """
    log_prefix = "å³æ™‚è«‹æ±‚" if is_immediate_push else "æ’ç¨‹æ¨æ’­"
    logging.info(f"[{log_prefix}] é–‹å§‹ç‚ºç”¨æˆ¶ {user_id} è™•ç†æ–°èè«‹æ±‚...")
    
    # æ±ºå®šå¿«å–çš„ key
    cache_key = user_custom_keywords if user_custom_keywords else "__DEFAULT__"
    current_time = time.time()

    # --- æ­¥é©Ÿ 1: æª¢æŸ¥å¿«å– ---
    if cache_key in NEWS_CACHE:
        cached_item = NEWS_CACHE[cache_key]
        cache_age = current_time - cached_item.get("timestamp", 0)
        
        if cache_age < NEWS_CACHE_DURATION_SECONDS:
            logging.info(f"æ–°èå¿«å–å‘½ä¸­ï¼(é—œéµå­—: '{cache_key}', å¹´é½¡: {int(cache_age)}ç§’)")
            cached_summary = cached_item.get("summary")
            if cached_summary:
                # åŠ ä¸Šä¸€å€‹æç¤ºï¼Œè®“ç”¨æˆ¶çŸ¥é“é€™æ˜¯å¿«å–çš„å…§å®¹
                cached_reply = f"ï¼ˆå¾å¿«å–æä¾›ğŸ˜Šï¼‰\n{cached_summary}"
                messages_to_send = handle_llm_response_with_think(cached_reply)
                send_line_messages(user_id, reply_token, messages_to_send)
                return # *** å¿«å–å‘½ä¸­ï¼Œæå‰çµæŸ ***

    logging.info(f"æ–°èå¿«å–æœªå‘½ä¸­æˆ–å·²éæœŸ (é—œéµå­—: '{cache_key}')ï¼ŒåŸ·è¡Œå®Œæ•´æ–°èæ‘˜è¦æµç¨‹ã€‚")

    # --- æ­¥é©Ÿ 2: å¦‚æœå¿«å–æœªå‘½ä¸­ï¼Œå‰‡åŸ·è¡Œå®Œæ•´æµç¨‹ ---
    articles = fetch_and_parse_articles(custom_query=user_custom_keywords, limit=NEWS_FETCH_TARGET_COUNT)
    if not articles:
        keywords_msg = f"ã€Œ{user_custom_keywords}ã€" if user_custom_keywords else "é è¨­ä¸»é¡Œ"
        send_line_messages(user_id, reply_token, [f"æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½æ ¹æ“šæ‚¨çš„é—œéµå­— {keywords_msg} æ‰¾åˆ°å¯æˆåŠŸæ“·å–çš„æ–°èã€‚"])
        return

    final_summary = summarize_news_flow(articles)
    if not final_summary or final_summary.startswith("æŠ±æ­‰ï¼Œ"):
        send_line_messages(user_id, reply_token, [final_summary or "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆç•°å¸¸ï¼Œå…§å®¹ç‚ºç©ºã€‚"])
        return

    # --- æ­¥é©Ÿ 3: å„²å­˜æ–°çš„å¿«å– ---
    NEWS_CACHE[cache_key] = {
        "timestamp": current_time,
        "summary": final_summary
    }
    save_json_data(NEWS_CACHE, NEWS_CACHE_FILE)
    logging.info(f"å·²æ›´æ–°æ–°èå¿«å– (é—œéµå­—: '{cache_key}')ã€‚")

    # --- æ­¥é©Ÿ 4: ç™¼é€æ–°ç”Ÿæˆçš„æ‘˜è¦çµ¦ç”¨æˆ¶ ---
    messages_to_send = handle_llm_response_with_think(final_summary)
    send_line_messages(user_id, reply_token, messages_to_send)
    
    logging.info(f"[{log_prefix}] å·²å®Œæˆå°ç”¨æˆ¶ {user_id} çš„æ–°èæ¨é€ã€‚")

def generate_news_for_single_user_job(user_id, keywords, remaining_users, is_immediate=False):
    with app.app_context():
        log_prefix = "èƒŒæ™¯å³æ™‚è«‹æ±‚" if is_immediate else "èƒŒæ™¯æ’ç¨‹æ¨æ’­"
        logging.info(f"[{log_prefix}] ä»»å‹™éˆå•Ÿå‹•ï¼Œç‚ºç”¨æˆ¶ {user_id} ç”¢ç”Ÿæ–°è...")
        try:
            generate_and_push_news_for_user(user_id=user_id, user_custom_keywords=keywords, is_immediate_push=is_immediate, reply_token=None)
        except Exception as e:
            logging.error(f"[{log_prefix}] èƒŒæ™¯ä»»å‹™ç‚ºç”¨æˆ¶ {user_id} ç”¢ç”Ÿæ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        finally:
            if remaining_users:
                next_user_id, next_user_keywords = remaining_users[0]
                next_remaining_users = remaining_users[1:]
                run_time = datetime.now(scheduler.timezone) + timedelta(seconds=10)
                job_id = f"scheduled_chain_{next_user_id}_{int(run_time.timestamp())}"
                scheduler.add_job(generate_news_for_single_user_job, 'date', run_date=run_time, args=[next_user_id, next_user_keywords, next_remaining_users, False], id=job_id)
                logging.info(f"ä»»å‹™éˆï¼šç‚ºç”¨æˆ¶ {user_id} çš„ä»»å‹™å·²å®Œæˆï¼Œå·²è¨»å†Šä¸‹ä¸€å€‹ä»»å‹™çµ¦ {next_user_id}ã€‚")
            else:
                logging.info(f"ä»»å‹™éˆï¼šç‚ºç”¨æˆ¶ {user_id} çš„ä»»å‹™å·²å®Œæˆï¼Œä»»å‹™éˆçµæŸã€‚")

@app.route('/webhook', methods=['POST'])
def webhook():
    signature = request.headers.get("X-Line-Signature")
    body_bytes = request.get_data()
    if not validate_signature(body_bytes, signature):
        logging.error("Webhook: Invalid signature.")
        return jsonify({"status": "invalid signature"}), 400
    try:
        data = request.json
        for event in data.get("events", []):
            source, event_type, reply_token = event.get("source", {}), event.get("type"), event.get("replyToken")
            source_type = source.get("type")
            context_id = source.get(f'{source_type}Id') if source_type else None
            if not context_id: continue
            logging.info(f"æ”¶åˆ°äº‹ä»¶: type={event_type}, source_type={source_type}, context_id={context_id}")
            if event_type == "message" and event.get("message", {}).get("type") == "text":
                handle_text_message_event(context_id=context_id, user_id=source.get('userId'), reply_token=reply_token, user_text=event["message"]["text"])
            elif event_type == "follow":
                user_pref = USER_PREFERENCES.get(context_id, {})
                user_pref["subscribed_news"] = True
                USER_PREFERENCES[context_id] = user_pref
                save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
                send_line_messages(context_id, reply_token, ["æ„Ÿè¬æ‚¨åŠ æˆ‘å¥½å‹ï¼è¼¸å…¥ `/bot å¹«åŠ©` å¯ä»¥æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤å–”ã€‚"])
            elif event_type == "unfollow" and context_id in USER_PREFERENCES:
                USER_PREFERENCES[context_id]["subscribed_news"] = False
                save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
        return jsonify({"status": "success"}), 200
    except Exception as e:
        logging.error(f"è™•ç† webhook æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500

def handle_llm_response_with_think(llm_full_response):
    """
    *** å·²ä¿®æ­£ v3ï¼šåªè² è²¬åˆ†å‰²å­—ä¸²ï¼Œä¸è² è²¬ç™¼é€ ***
    è¿”å›ä¸€å€‹æº–å‚™è¦ç™¼é€çš„è¨Šæ¯åˆ—è¡¨ã€‚
    """
    think_pattern = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
    match = think_pattern.search(llm_full_response)
    
    messages_to_send = []
    
    if match:
        thinking_text = match.group(1).strip()
        formal_text = llm_full_response[match.end():].strip()
        
        if thinking_text:
            # messages_to_send.extend(split_long_message(f"âš™ï¸ æˆ‘çš„æ€è€ƒéç¨‹ï¼š\n{thinking_text}"))
            # ä¸ç™¼é€æ€è€ƒéç¨‹ æ€è€ƒéç¨‹æ˜¯å¦ç™¼é€æˆ–è¨±æ”¹æˆè®Šæ•¸æ§åˆ¶
            pass
        if formal_text:
            messages_to_send.extend(split_long_message(formal_text))
    else:
        messages_to_send.extend(split_long_message(llm_full_response))
        
    return messages_to_send

def handle_text_message_event(context_id, user_id, reply_token, user_text):
    """
    *** å…¨æ–°é‡æ§‹ v4ï¼šæ¡ç”¨æ··åˆæ¨¡å¼è™•ç†æ–°èè«‹æ±‚ ***
    ä¸€æ¬¡æ€§æŸ¥è©¢æ”¹ç‚ºåŒæ­¥è™•ç†ï¼Œä»¥æœ€å¤§åŒ–åˆ©ç”¨ reply_tokenã€‚
    """
    # ... (è¨˜éŒ„æ­·å²çš„é‚è¼¯ä¿æŒä¸è®Š) ...
    display_name = get_user_profile(context_id, user_id)
    if context_id.startswith(('G', 'R')): formatted_message_content = f"{display_name}: {user_text}"
    else: formatted_message_content = user_text
    history = CONVERSATION_HISTORY.get(context_id, []); history.append({"role": "user", "content": formatted_message_content})
    if len(history) > MAX_HISTORY_MESSAGES: history = history[-MAX_HISTORY_MESSAGES:]
    CONVERSATION_HISTORY[context_id] = history
    save_json_data(CONVERSATION_HISTORY, CONVERSATION_HISTORY_FILE)
    logging.info(f"å·²è¨˜éŒ„è¨Šæ¯åˆ° {context_id}ã€‚ç•¶å‰æ­·å²é•·åº¦: {len(history)}")

    user_text_stripped = user_text.strip()
    if not user_text_stripped.startswith(BOT_TRIGGER_WORD): return

    command_text = user_text_stripped[len(BOT_TRIGGER_WORD):].strip()
    if not command_text or command_text.lower() in ["help", "å¹«åŠ©", "æŒ‡ä»¤"]:
        send_line_messages(context_id, reply_token, [HELP_MESSAGE.strip()]); return

    cmd_parts = command_text.lower().split()
    main_command = cmd_parts[0] if cmd_parts else ""

    # --- 1. æ–°èä¸€æ¬¡æ€§æŸ¥è©¢ (æ”¹ç‚ºåŒæ­¥åŸ·è¡Œ) ---
    if main_command in ["æ–°è", "news", "æ–°èæ‘˜è¦"]:
        logging.info("åµæ¸¬åˆ°ã€Œæ–°èä¸€æ¬¡æ€§æŸ¥è©¢ã€æŒ‡ä»¤ (åŒæ­¥æ¨¡å¼)ã€‚")
        keyword_part = command_text[len(main_command):].strip()
        keywords = None
        if keyword_part.lower().startswith("é—œéµå­—:"):
            keywords = keyword_part[len("é—œéµå­—:"):].strip()
            if not keywords: keywords = None
        
        # *** ä¿®æ”¹æ ¸å¿ƒ ***
        # ä¸å†è¨»å†ŠèƒŒæ™¯ä»»å‹™ï¼Œè€Œæ˜¯ç›´æ¥å‘¼å«æ–°èè™•ç†å‡½å¼ï¼Œä¸¦å‚³å…¥ reply_token
        generate_and_push_news_for_user(
            user_id=context_id,
            user_custom_keywords=keywords,
            is_immediate_push=True,
            reply_token=reply_token
        )

    # --- 2. æŒä¹…åŒ–è¨‚é–±ç®¡ç† (ä¿æŒä¸è®Š) ---
    elif main_command == "è¨‚é–±":
        # ... (æ­¤è™•é‚è¼¯ä¸è®Š) ...
        logging.info("åµæ¸¬åˆ°ã€Œè¨‚é–±ã€æŒ‡ä»¤ã€‚")
        keywords_to_subscribe = command_text[len(main_command):].strip()
        user_pref = USER_PREFERENCES.get(context_id, {})
        user_pref["subscribed_news"] = True
        user_pref["news_keywords"] = keywords_to_subscribe or None
        reply_msg = f"âœ… è¨­å®šæˆåŠŸï¼å·²ç‚ºæ‚¨è¨‚é–±æ¯æ—¥æ–°èï¼Œä¸»é¡Œç‚ºï¼šã€Œ{keywords_to_subscribe or 'é è¨­ AI ä¸»é¡Œ'}ã€ã€‚"
        USER_PREFERENCES[context_id] = user_pref
        save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
        send_line_messages(context_id, reply_token, [reply_msg])
    
    # ... (æŸ¥çœ‹è¨‚é–±ã€å–æ¶ˆè¨‚é–±çš„ elif å€å¡Šä¿æŒä¸è®Š) ...
    elif main_command == "æŸ¥çœ‹è¨‚é–±":
        user_pref = USER_PREFERENCES.get(context_id, {}); reply_msg = "æ‚¨ç›®å‰å°šæœªè¨‚é–±æ¯æ—¥æ–°èå–”ã€‚"
        if user_pref.get("subscribed_news"): subscribed_keywords = user_pref.get("news_keywords", "é è¨­ AI ä¸»é¡Œ"); reply_msg = f"æ‚¨ç›®å‰çš„è¨‚é–±ç‹€æ…‹ç‚ºï¼š\n- ç‹€æ…‹ï¼šå·²è¨‚é–± âœ…\n- ä¸»é¡Œï¼šã€Œ{subscribed_keywords}ã€"
        send_line_messages(context_id, reply_token, [reply_msg])
    elif main_command == "å–æ¶ˆè¨‚é–±":
        user_pref = USER_PREFERENCES.get(context_id, {}); user_pref["subscribed_news"] = False; USER_PREFERENCES[context_id] = user_pref
        save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE); send_line_messages(context_id, reply_token, ["â˜‘ï¸ å¥½çš„ï¼Œå·²ç‚ºæ‚¨å–æ¶ˆæ¯æ—¥æ–°èè¨‚é–±ã€‚"])


    # --- 3. ä¸€èˆ¬èŠå¤© (ä¿æŒä¸è®Š) ---
    else:
        logging.info("ä½œç‚ºä¸€èˆ¬èŠå¤©å•é¡Œè™•ç†ã€‚")
        llm_response = generate_chat_response(context_id, command_text)
        if not llm_response.startswith("æŠ±æ­‰ï¼Œ"):
            think_pattern = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)
            cleaned_bot_response = re.sub(think_pattern, '', llm_response).strip()
            history.append({"role": "assistant", "content": cleaned_bot_response})
            if len(history) > MAX_HISTORY_MESSAGES: history = history[-MAX_HISTORY_MESSAGES:]
            CONVERSATION_HISTORY[context_id] = history
            save_json_data(CONVERSATION_HISTORY, CONVERSATION_HISTORY_FILE)
        
        # å‘¼å«ä¿®æ­£å¾Œçš„ç™¼é€æµç¨‹
        messages_to_send = handle_llm_response_with_think(llm_response)
        send_line_messages(context_id, reply_token, messages_to_send)            

# ==============================================================================
# --- æ’ç¨‹èˆ‡æ‡‰ç”¨å•Ÿå‹• ---
# ==============================================================================
scheduler = BackgroundScheduler(timezone="Asia/Taipei", daemon=True)

def daily_news_push_job():
    with app.app_context():
        logging.info("APScheduler: ä»»å‹™éˆå•Ÿå‹•å™¨é–‹å§‹åŸ·è¡Œ...")
        users_to_push = [(uid, prefs.get("news_keywords")) for uid, prefs in load_json_data(USER_PREFERENCES_FILE).items() if prefs.get("subscribed_news")]
        if TARGET_USER_ID_FOR_TESTING and not any(u[0] == TARGET_USER_ID_FOR_TESTING for u in users_to_push):
            users_to_push.append((TARGET_USER_ID_FOR_TESTING, load_json_data(USER_PREFERENCES_FILE).get(TARGET_USER_ID_FOR_TESTING, {}).get("news_keywords")))
        if not users_to_push:
            logging.info("APScheduler: å•Ÿå‹•å™¨ç™¼ç¾æ²’æœ‰éœ€è¦è™•ç†çš„ç”¨æˆ¶ã€‚")
            return
        logging.info(f"APScheduler: å•Ÿå‹•å™¨æº–å‚™å•Ÿå‹•ä¸€å€‹åŒ…å« {len(users_to_push)} ä½ç”¨æˆ¶çš„ä»»å‹™éˆã€‚")
        first_user_id, first_user_keywords = users_to_push[0]
        remaining_users = users_to_push[1:]
        job_id = f"scheduled_chain_{first_user_id}_{int(time.time())}"
        scheduler.add_job(generate_news_for_single_user_job, 'date', run_date=datetime.now(scheduler.timezone) + timedelta(seconds=5), args=[first_user_id, first_user_keywords, remaining_users, False], id=job_id)
        logging.info(f"APScheduler: ä»»å‹™éˆçš„ç¬¬ä¸€å€‹ä»»å‹™å·²è¨»å†Šçµ¦ {first_user_id}ï¼Œå•Ÿå‹•å™¨ä»»å‹™çµæŸã€‚")

def shutdown_scheduler_on_exit():
    if scheduler.running: scheduler.shutdown(wait=False)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Line Bot and News Fetcher")
    parser.add_argument('--test-news', action='store_true', help='Run in local test mode for news fetching and summarization.')
    parser.add_argument('--keywords', type=str, default=None, help='Keywords for news fetching in test mode.')
    parser.add_argument('--limit', type=int, default=None, help='Number of articles to process in test mode.')
    args = parser.parse_args()

    if args.test_news:
        def run_test_mode(keywords, limit):
            print("="*50 + "\nğŸš€ é€²å…¥æœ¬åœ°æ¸¬è©¦æ¨¡å¼ ğŸš€\n" + "="*50)
            articles = fetch_and_parse_articles(custom_query=keywords, limit=limit or NEWS_FETCH_TARGET_COUNT)
            if not articles:
                print("[!] æ¸¬è©¦ä¸­æ­¢ï¼šæœªèƒ½æˆåŠŸæ“·å–ä»»ä½•æ–°èå…§æ–‡ã€‚")
                return
            final_summary = summarize_news_flow(articles)
            print("\n" + "="*50 + "\nğŸ§ æœ€çµ‚ Podcast é¢¨æ ¼æ‘˜è¦ ğŸ§\n" + "="*50)
            print(final_summary)
            print("\n" + "="*50 + "\nâœ… æ¸¬è©¦æµç¨‹çµæŸ âœ…\n" + "="*50)
        run_test_mode(args.keywords, args.limit)
    else:
        logging.info("ğŸš€ å•Ÿå‹• Flask Web ä¼ºæœå™¨æ¨¡å¼ ğŸš€")
        required_env_vars = ['LINE_CHANNEL_ACCESS_TOKEN', 'LINE_CHANNEL_SECRET', 'OPENAI_API_KEY']
        if any(not os.getenv(var) for var in required_env_vars):
            logging.critical(f"CRITICAL: Missing required environment variables: {', '.join(v for v in required_env_vars if not os.getenv(v))}. Exiting.")
            exit(1)
        if not scheduler.get_jobs():
            scheduler.add_job(daily_news_push_job, 'cron', hour=9, minute=0, id='daily_news_cron', replace_existing=True)
            scheduler.add_job(daily_news_push_job, 'interval', minutes=480, id='news_interval_job', replace_existing=True)
            logging.info("å·²è¨­å®šæ¯æ—¥ 09:00 å’Œæ¯ 480 åˆ†é˜çš„æ–°èæ¨æ’­æ’ç¨‹ã€‚")
            if os.getenv("RUN_JOB_ON_STARTUP", "False").lower() == "true":
                scheduler.add_job(daily_news_push_job, 'date', run_date=datetime.now(scheduler.timezone) + timedelta(seconds=15), id='startup_news_push')
                logging.info(f"å·²è¨­å®šåœ¨ 15 ç§’å¾ŒåŸ·è¡Œä¸€æ¬¡æ–°èæ¨æ’­ä»»å‹™ã€‚")
        if not scheduler.running:
            scheduler.start()
            logging.info("APScheduler started.")
            atexit.register(shutdown_scheduler_on_exit)
        port = int(os.environ.get("PORT", 5000))
        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)