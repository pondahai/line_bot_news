# ==============================================================================
# line_bot_v5_2.py
# å¾ªåºåŸ·è¡Œå¯¦é©—ç‰ˆæœ¬
#
# ç‰ˆæœ¬äº®é» (v5 -> v5_2):
# - æ–°èæŠ“å–æµç¨‹é‡æ§‹ï¼šç”±å¹³è¡Œè™•ç†æ”¹ç‚ºå¾ªåºåŸ·è¡Œã€‚
# - å¯¦æ–½ã€Œå·¥ä½œç¯„ç–‡ã€çš„ Selenium å¯¦ä¾‹ç®¡ç†æ¨¡å¼ã€‚
# - åœ¨å–®æ¬¡æ–°èæŠ“å–ä»»å‹™ä¸­ï¼Œåªå•Ÿå‹•ä¸€æ¬¡ç€è¦½å™¨ï¼Œä¸¦é‡è¤‡ä½¿ç”¨è©²å¯¦ä¾‹è™•ç†æ‰€æœ‰æ–‡ç« ã€‚
# - æ­¤ä¿®æ”¹æ—¨åœ¨é€éé¿å…åè¦†å•Ÿå‹•ç€è¦½å™¨çš„é–‹éŠ·ä¾†æå‡æ•ˆèƒ½ï¼Œä¸¦é™ä½ç³»çµ±è³‡æºçš„ç¬é–“è² è¼‰ã€‚
# ==============================================================================

# --- Python Standard Libraries ---
import os
import platform
import sys
import time
import uuid
import json
import logging
import hashlib
import hmac
import base64
import re
import atexit
import argparse
from datetime import datetime, timedelta, timezone
import urllib.parse
from urllib.parse import urlparse, parse_qs, unquote


# --- Third-party Libraries ---
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from apscheduler.schedulers.background import BackgroundScheduler
import feedparser

# --- Newspaper3k for article scraping ---
from newspaper import Article, Config

# --- Selenium for dynamic content scraping ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException, TimeoutException

# (åœ¨æª”æ¡ˆé ‚éƒ¨ï¼Œèˆ‡å…¶ä»– import æ”¾åœ¨ä¸€èµ·)
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==============================================================================
# --- ç’°å¢ƒè¨­å®šã€æ—¥èªŒèˆ‡ Flask åˆå§‹åŒ– ---
# ==============================================================================
load_dotenv()
app = Flask(__name__)

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]',
    stream=sys.stdout
)

# --- å…¨åŸŸè®Šæ•¸èˆ‡å¸¸æ•¸ ---
BOT_TRIGGER_WORD = os.getenv("BOT_TRIGGER_WORD", "/bot")
OPENAI_COMPLETION_MODEL = os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com")
TARGET_USER_ID_FOR_TESTING = os.getenv("TARGET_USER_ID_FOR_TESTING")

VISUAL_SEPARATION_DELAY = float(os.getenv("VISUAL_SEPARATION_DELAY", "1.0"))
DEFAULT_NEWS_KEYWORDS = "å¤§å‹èªè¨€æ¨¡å‹ OR LLM OR ç”Ÿæˆå¼AI OR OpenAI OR Gemini OR Claude"
USER_PREFERENCES_FILE = "user_preferences.json"
CONVERSATION_HISTORY_FILE = "conversation_history.json"
MAX_HISTORY_MESSAGES = int(os.getenv("MAX_HISTORY_MESSAGES", "50"))
NEWS_FETCH_TARGET_COUNT = 6
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'

NEWS_CACHE_FILE = "news_cache.json"
NEWS_SUMMARY_CACHE_SECONDS = 3600 * 4  # 4 å°æ™‚

# --- ç”¨æˆ¶å€‹äººè³‡æ–™å¿«å– (in-memory) ---
USER_PROFILE_CACHE = {}
USER_PROFILE_CACHE_SECONDS = 7200  # å¿«å– 2 å°æ™‚

MAX_MESSAGE_LENGTH = int(os.getenv("MAX_MESSAGE_LENGTH", "4800"))

# --- å…©éšæ®µæ‘˜è¦çš„ LLM Prompt è¨­å®š ---
PROMPT_FOR_INDIVIDUAL_SUMMARY = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æ–°èç·¨è¼¯ï¼Œå°ˆé•·æ˜¯å¿«é€Ÿæç…‰æ–‡ç« æ ¸å¿ƒã€‚è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–°èå…§æ–‡ï¼Œæ¿ƒç¸®æˆä¸€æ®µä¸è¶…é150å­—çš„å®¢è§€ã€ç²¾ç°¡ä¸­æ–‡æ‘˜è¦ã€‚"
    "æ‘˜è¦æ‡‰åŒ…å«æœ€é—œéµçš„äººç‰©ã€äº‹ä»¶ã€æ•¸æ“šå’Œçµè«–ã€‚è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹é ­æˆ–çµå°¾çš„å®¢å¥—è©±ã€‚"
)

current_date = datetime.now().strftime("%Y-%m-%d %H:%M")
PROMPT_FOR_FINAL_AGGREGATION = (
    f"ä»Šå¤©æ—¥æœŸæ˜¯ {current_date}ã€‚\n"
    "ä½ æ˜¯ä¸€ä½é¢¨è¶£å¹½é»˜ã€çŸ¥è­˜æ·µåšçš„æ–°è Podcast ä¸»æŒäººã€‚ä½ çš„è½çœ¾æ˜¯ Line ç”¨æˆ¶ï¼Œä»–å€‘å–œæ­¡è¼•é¬†ã€æ˜“æ‡‚ä¸”å¸¶æœ‰ Emoji çš„å…§å®¹ã€‚"
    "æ¥ä¸‹ä¾†æˆ‘æœƒæä¾›æ•¸å‰‡ã€Œé™„æœ‰ç™¼å¸ƒæ—¥æœŸçš„ç²¾ç°¡æ–°èæ‘˜è¦ã€ã€‚è«‹ä½ æ ¹æ“šé€™äº›æ‘˜è¦ï¼Œç™¼æ®ä½ çš„ä¸»æŒé¢¨æ ¼ï¼Œå°‡å®ƒå€‘æ•´åˆæˆä¸€ç¯‡é€£è²«çš„è«‡è©±æ€§å…§å®¹ã€‚"
    "ä½ çš„ä»»å‹™æ˜¯ï¼š\n"
    "1. ç”¨ç”Ÿå‹•çš„èªæ°£é–‹å ´ï¼Œå¸å¼•è½çœ¾æ³¨æ„ã€‚\n"
    "2. å°‡å„å‰‡æ–°èæ‘˜è¦è‡ªç„¶åœ°ä¸²é€£èµ·ä¾†ï¼Œä½ å¯ä»¥æ ¹æ“šæ–°èçš„ç™¼å¸ƒæ—¥æœŸï¼ˆä¾‹å¦‚ä½¿ç”¨ã€æ˜¨å¤©ã€ã€ã€ä»Šå¤©æ—©ä¸Šã€ç­‰è©å½™ï¼‰ä¾†å¢åŠ æ™‚æ•ˆæ„Ÿï¼Œä½†ä¸è¦æœæ’°ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚\n"
    "3. åœ¨æåˆ°æ¯å‰‡æ–°èçš„é‡é»å¾Œï¼Œè«‹å‹™å¿…é™„ä¸Šé€™å‰‡æ–°èçš„åŸå§‹æ¨™é¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
    "   - æ¨™é¡Œï¼š[åŸå§‹æ–°èæ¨™é¡Œ] - ç™¼å¸ƒæ™‚é–“ï¼š[æ–°èç™¼å¸ƒæ™‚é–“]\n"
    "4. å…¨ç¨‹å¤šä½¿ç”¨ Emoji ä¾†å¢åŠ æ´»æ½‘æ„Ÿã€‚\n"
    "5. è¦åš´è‚…æ‡‰å°æ¯å‰‡æœ‰è² é¢æƒ…ç·’çš„æ–°èä¾‹å¦‚ç½é›£èˆ‡å‚·äº¡ã€‚\n"
    "6. æœ€å¾Œçµè«–è¦åŠ è¨»é€™æ˜¯AIç”Ÿæˆçš„å…§å®¹ï¼Œè®€è€…æ‡‰æ³¨æ„æ­£ç¢ºæ€§ã€‚\n"
    "7. ç¸½çµçš„å›ç­”å­—æ•¸é™åˆ¶åœ¨500å­—ä»¥ä¸‹ä»¥ç¬¦åˆé€šè¨Šè»Ÿé«”çš„é™åˆ¶ã€‚\n"
)

# --- æ©Ÿå™¨äººæŒ‡ä»¤å¹«åŠ©è¨Šæ¯ ---
HELP_MESSAGE = """
å“ˆå›‰ï¼ğŸ‘‹ æˆ‘æ˜¯ä½ çš„ AI åŠ©ç†ï¼

ä½ å¯ä»¥é€é `/bot` æŒ‡ä»¤èˆ‡æˆ‘äº’å‹•ã€‚

ğŸ“°ã€æ–°èåŠŸèƒ½ã€‘
ğŸ”¹ `/bot æ–°è`
   ç«‹å³å–å¾—ä¸€ç¯‡ AI ä¸»é¡Œçš„æ–°èæ‘˜è¦ã€‚
ğŸ”¹ `/bot æ–°è é—œéµå­—:ä½ æƒ³çœ‹çš„å…§å®¹`
   ç«‹å³æŸ¥è©¢ç‰¹å®šä¸»é¡Œçš„æ–°èã€‚
ğŸ”¹ `/bot è¨‚é–±`
   è¨‚é–±æ¯æ—¥ AI æ–°èæ¨æ’­ã€‚
ğŸ”¹ `/bot è¨‚é–± [ä½ çš„ä¸»é¡Œ]`
   è¨‚é–±æ¯æ—¥ç‰¹å®šä¸»é¡Œçš„æ–°èã€‚
ğŸ”¹ `/bot æŸ¥çœ‹è¨‚é–±`
   çœ‹çœ‹ä½ ç›®å‰è¨‚é–±äº†ä»€éº¼ã€‚
ğŸ”¹ `/bot å–æ¶ˆè¨‚é–±`
   å–æ¶ˆæ¯æ—¥æ–°èæ¨æ’­ã€‚

ğŸ’¬ã€éš¨æ„èŠå¤©ã€‘
é™¤äº†æ–°èï¼Œä¹Ÿå¯ä»¥éš¨æ™‚ç”¨ `/bot` å•æˆ‘ä»»ä½•å•é¡Œå–”ï¼
ç¯„ä¾‹ï¼š`/bot å¹«æˆ‘è¦åŠƒä¸€ä¸‹é€±æœ«è¡Œç¨‹`
"""

# ==============================================================================
# --- æ–°èæ“·å–æ¨¡çµ„ (v5_2 Refactored) ---
# ==============================================================================
newspaper_config = Config()
newspaper_config.browser_user_agent = USER_AGENT
newspaper_config.request_timeout = 15
newspaper_config.memoize_articles = False

# --- Selenium Options (é›†ä¸­ç®¡ç†) ---
chrome_options = Options()
# åœ¨ Docker æˆ–ç„¡ GUI ç’°å¢ƒä¸­ï¼Œå‹™å¿…å•Ÿç”¨ headless
if os.getenv("SELENIUM_HEADLESS", "true").lower() == "true":
    chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--window-size=1280,2400")
chrome_options.page_load_strategy = "eager" # åŠ å¿«é é¢è¿”å›é€Ÿåº¦
chrome_options.add_experimental_option("prefs", {
    "profile.managed_default_content_settings.images": 2, # ä¸è¼‰å…¥åœ–ç‰‡
})

# --- Selenium Helper Functions ---

def _dom_is_stable(driver, min_text_len=500, settle_checks=3, interval=0.6, overall_timeout=20):
    """
    å›å‚³ True ç•¶ DOM æ–‡å­—é•·åº¦ç©©å®šï¼ˆé€£çºŒ settle_checks æ¬¡å¹¾ä¹ä¸å†æˆé•·ï¼‰ï¼Œ
    ä¸¦ä¸”é•·åº¦è¶…é min_text_lenã€‚é¿å…åœ¨ SPA/React é‚„åœ¨æ›è¼‰æ™‚å°±è®€ç©ºç™½ã€‚
    """
    t0 = time.time()
    last_len = -1
    stable = 0
    while time.time() - t0 < overall_timeout:
        try:
            txt_len = driver.execute_script("return (document.body && document.body.innerText) ? document.body.innerText.length : 0;")
        except WebDriverException:
            txt_len = 0
        if last_len >= 0 and abs(txt_len - last_len) < 30 and txt_len >= min_text_len:
            stable += 1
            if stable >= settle_checks:
                return True
        else:
            stable = 0
        last_len = txt_len
        time.sleep(interval)
    return False

def _get_outer_html(driver):
    try:
        return driver.execute_script("return document.documentElement ? document.documentElement.outerHTML : ''") or ""
    except WebDriverException:
        return ""

def _try_all_iframes_html(driver, max_frames=10):
    """
    æœ‰äº›æ–°èç«™æŠŠæ­£æ–‡æ”¾åœ¨ iframeã€‚é€™è£¡æœƒæŠŠæ‰€æœ‰ iframe outerHTML æ‹¼èµ·ä¾†ã€‚
    è‹¥è·¨ç¶²åŸŸä¸èƒ½é€²å…¥æŸäº› iframeï¼Œæœƒè‡ªå‹•è·³éã€‚
    """
    htmls = []
    try:
        frames = driver.find_elements(By.CSS_SELECTOR, "iframe")
    except WebDriverException:
        frames = []
    if not frames:
        return ""
    for idx, frame in enumerate(frames[:max_frames]):
        try:
            driver.switch_to.frame(frame)
            time.sleep(0.2)
            try:
                WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            except Exception:
                pass
            _dom_is_stable(driver, min_text_len=200, settle_checks=2, interval=0.4, overall_timeout=6)
            htmls.append(_get_outer_html(driver))
        except Exception:
            pass
        finally:
            driver.switch_to.default_content()
    return "\n<!-- IFRAME_JOIN_BOUNDARY -->\n".join([h for h in htmls if h])

def _get_page_html_with_driver(driver: webdriver.Chrome, url: str, min_text_len: int = 700) -> str:
    """
    ä½¿ç”¨ä¸€å€‹ã€å·²å­˜åœ¨ã€‘çš„ driver å¯¦ä¾‹ä¾†æŠ“å–æŒ‡å®š URL çš„ HTML å…§å®¹ã€‚
    é€™æ˜¯ v5_2 é‡æ§‹å¾Œçš„æ ¸å¿ƒ Selenium äº’å‹•å‡½å¼ã€‚
    """
    logging.info(f"    [Selenium] Using existing driver to fetch: {url[:70]}...")
    driver.get(url)

    try:
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    except TimeoutException:
        logging.warning("    [Selenium] Timed out waiting for <body> tag.")
        # å³ä½¿è¶…æ™‚ï¼Œæˆ‘å€‘é‚„æ˜¯å˜—è©¦æŠ“å–å…§å®¹

    try:
        driver.execute_script("window.scrollTo(0, 600);")
        time.sleep(0.2)
        driver.execute_script("window.scrollTo(0, 0);")
    except Exception:
        pass

    _ = _dom_is_stable(driver, min_text_len=min_text_len, settle_checks=3, interval=0.6, overall_timeout=25)

    html = _get_outer_html(driver)
    text_len = 0
    try:
        text_len = driver.execute_script("return (document.body && document.body.innerText) ? document.body.innerText.length : 0;")
    except Exception:
        pass

    if text_len < min_text_len:
        logging.info(f"    [Selenium] Page text length ({text_len}) is short, trying to extract from iframes.")
        iframe_html = _try_all_iframes_html(driver, max_frames=12)
        if iframe_html:
            html = html + "\n<!-- CONCAT IFRAME HTML BELOW -->\n" + iframe_html

    logging.info(f"    [Selenium] Fetched from {driver.current_url}. Main text length: {text_len}.")
    return html or ""


def get_real_url(google_news_url):
    try:
        headers = {"User-Agent": USER_AGENT}
        with requests.get(google_news_url, headers=headers, allow_redirects=True, timeout=20, stream=True) as r:
            return r.url
    except requests.RequestException as e:
        logging.warning(f"[éŒ¯èª¤] è§£æè·³è½‰é€£çµå¤±æ•— {google_news_url}: {e}")
        # å¦‚æœè«‹æ±‚å¤±æ•—ï¼Œå˜—è©¦å¾ URL åƒæ•¸è§£æ
        try:
            pu = urlparse(google_news_url)
            if pu.netloc.endswith("news.google.com"):
                qs = parse_qs(pu.query)
                if "url" in qs and qs["url"]:
                    return unquote(qs["url"][0])
        except Exception:
            pass
        return google_news_url # è¿”å›åŸå§‹ URL ä½œç‚ºå‚™æ´

def fetch_and_parse_articles(custom_query=None, limit=NEWS_FETCH_TARGET_COUNT):
    """
    *** v5_2 ç‰ˆæœ¬ï¼šæ¡ç”¨å¾ªåºåŸ·è¡Œï¼Œé‡è¤‡ä½¿ç”¨å–®ä¸€ Selenium å¯¦ä¾‹ä¾†åŠ é€Ÿ ***
    """
    query_to_use = custom_query.strip() if custom_query and custom_query.strip() else DEFAULT_NEWS_KEYWORDS
    encoded_query = urllib.parse.quote_plus(query_to_use)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    logging.info(f">>> é–‹å§‹å¾ Google News RSS å–å¾—æ–°èåˆ—è¡¨ (é—œéµå­—: '{query_to_use}')")
    feed = feedparser.parse(rss_url)

    if feed.bozo:
        logging.error(f"ç„¡æ³•è§£æ RSS feedã€‚éŒ¯èª¤è³‡è¨Š: {feed.bozo_exception}")
        return []

    successful_articles = []
    processed_urls = set()
    entries_to_process = feed.entries[:limit * 2]

    # --- å¾ªåºè™•ç†æ ¸å¿ƒ ---
    # åœ¨æ‰€æœ‰ä»»å‹™é–‹å§‹å‰ï¼Œåªå•Ÿå‹•ä¸€æ¬¡ç€è¦½å™¨ã€‚
    # ä½¿ç”¨ "with" é™³è¿°å¼ç¢ºä¿ç€è¦½å™¨åœ¨å€å¡ŠçµæŸå¾Œä¸€å®šæœƒè¢«é—œé–‰ã€‚
    logging.info("åˆå§‹åŒ–å–®ä¸€ç€è¦½å™¨å¯¦ä¾‹ä»¥ä¾›æœ¬æ¬¡ä»»å‹™é‡è¤‡ä½¿ç”¨...")
    try:
        with webdriver.Chrome(options=chrome_options) as driver:
            driver.set_page_load_timeout(60)
            driver.set_script_timeout(60)
            logging.info("ç€è¦½å™¨å¯¦ä¾‹å·²å•Ÿå‹•ï¼Œé–‹å§‹å¾ªåºè™•ç†æ–°èæ¢ç›®ã€‚")
            
            for i, entry in enumerate(entries_to_process):
                if len(successful_articles) >= limit:
                    logging.info("å·²é”åˆ°ç›®æ¨™æ–°èæ•¸é‡ï¼Œæå‰çµæŸæŠ“å–ã€‚")
                    break

                logging.info(f"  [å¾ªåºè™•ç† {i+1}/{len(entries_to_process)}] é–‹å§‹è™•ç†: {entry.title}")
                real_url = get_real_url(entry.link)
                if not real_url or real_url in processed_urls:
                    logging.warning(f"  è·³é: ç„¡æ³•å–å¾—çœŸå¯¦ URL æˆ– URL é‡è¤‡ for {entry.title}")
                    continue
                
                try:
                    article = Article(real_url, language='zh', config=newspaper_config)
                    article.download()
                    article.parse()
                    
                    if len(article.text) < 200:
                        logging.warning(f"  å…§å®¹éçŸ­ï¼Œç‚º '{entry.title}' å•Ÿç”¨ Selenium å‚™æ´æŠ“å–ã€‚")
                        html_content = _get_page_html_with_driver(driver, real_url)
                        if html_content:
                            article.download(input_html=html_content)
                            article.parse()

                    if article.title and len(article.text) > 50:
                        publish_date = None
                        # å„ªå…ˆä½¿ç”¨ newspaper3k å¾ç¶²é è§£æçš„æ—¥æœŸï¼Œé€šå¸¸æ›´æº–ç¢º
                        if hasattr(article, 'publish_date') and article.publish_date:
                            publish_date = article.publish_date.astimezone() # è½‰æ›ç‚ºå¸¶æœ‰æœ¬åœ°æ™‚å€çš„ datetime ç‰©ä»¶
                        # å¦‚æœç¶²é ä¸Šæ²’æœ‰æ—¥æœŸï¼Œä½¿ç”¨ RSS feed çš„ pubDate ä½œç‚ºå‚™æ´
                        elif hasattr(entry, 'published_parsed') and entry.published_parsed:
                            # entry.published_parsed æ˜¯ time.struct_timeï¼Œéœ€è¦è½‰æ›
                            # æ³¨æ„ï¼šåŸå§‹æ™‚é–“æ˜¯ GMTï¼Œæˆ‘å€‘éœ€è¦è™•ç†æ™‚å€
                            dt_gmt = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
                            publish_date = dt_gmt.astimezone() # è½‰æ›ç‚ºæœ¬åœ°æ™‚å€
                            
                        logging.info(f"  æˆåŠŸå–å¾—: {article.title} (ç™¼å¸ƒæ–¼: {publish_date.strftime('%Y-%m-%d %H:%M') if publish_date else 'æœªçŸ¥'})")
                        successful_articles.append({
                            'title': article.title,
                            'text': article.text,
                            'url': real_url,
                            'source': entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else "æœªçŸ¥ä¾†æº",
                            'publish_date': publish_date  # å°‡æ—¥æœŸç‰©ä»¶å„²å­˜èµ·ä¾†
                        })
                        processed_urls.add(real_url)
                    else:
                        logging.warning(f"  å¤±æ•—: ç„¡æ³•ç‚º {entry.title} è§£æè¶³å¤ å…§æ–‡ã€‚")
                except Exception as e:
                    logging.error(f"  è™•ç† {entry.title} æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=False) # exc_info=False é¿å…éå¤šæ—¥èªŒ

    except WebDriverException as e:
        logging.critical(f"WebDriver å¯¦ä¾‹å•Ÿå‹•æˆ–åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œæœ¬æ¬¡æŠ“å–ä¸­æ­¢: {e}", exc_info=True)
        # å¦‚æœ driver å•Ÿå‹•å¤±æ•—ï¼Œå›å‚³ç©ºåˆ—è¡¨
        return []

    logging.info(f">>> å¾ªåºæ–°èå…§æ–‡æ“·å–å®Œæˆï¼Œå…±æˆåŠŸå–å¾— {len(successful_articles)} ç¯‡ã€‚")
    # --- æ–°å¢çš„éæ¿¾èˆ‡æ’åºé‚è¼¯ ---
    if successful_articles:
        now = datetime.now().astimezone()
        # å¯ä»¥å°‡å¤©æ•¸è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸ï¼Œä¾‹å¦‚ 3 å¤©
        days_limit = int(os.getenv("NEWS_FETCH_DAYS_LIMIT", "3"))
        time_threshold = now - timedelta(days=days_limit)
        
        original_count = len(successful_articles)
        
        # 1. éæ¿¾æ‰æ²’æœ‰æ—¥æœŸæˆ–å¤ªèˆŠçš„æ–‡ç« 
        successful_articles = [
            art for art in successful_articles 
            if art.get('publish_date') and art['publish_date'] > time_threshold
        ]
        
        # 2. æ ¹æ“šç™¼å¸ƒæ—¥æœŸç”±æ–°åˆ°èˆŠæ’åº
        successful_articles.sort(key=lambda x: x.get('publish_date'), reverse=True)
        
        filtered_count = len(successful_articles)
        logging.info(f"æ—¥æœŸéæ¿¾å®Œæˆ: å¾ {original_count} ç¯‡ç¯©é¸å‡º {filtered_count} ç¯‡è¿‘ {days_limit} å¤©å…§çš„æ–°èã€‚")
        
    return successful_articles[:limit]

def _extract_assistant_text_from_response(resp_json: dict) -> str:
    if not resp_json or "choices" not in resp_json or not resp_json["choices"]:
        return ""
    ch0 = resp_json["choices"][0]
    msg = ch0.get("message", {}) or {}

    # 1) content å¯èƒ½æ˜¯å­—ä¸²æˆ– parts
    content = msg.get("content")
    if isinstance(content, str) and content.strip():
        return content.strip()
    if isinstance(content, list):
        parts = []
        for p in content:
            if isinstance(p, str):
                parts.append(p)
            elif isinstance(p, dict):
                txt = p.get("text") or p.get("output_text") or p.get("data") or p.get("value") or ""
                if txt: parts.append(txt)
        if parts:
            return "".join(parts).strip()

    # 2) å…¼å®¹èˆŠçš„ choices[0].text
    legacy = ch0.get("text")
    if isinstance(legacy, str) and legacy.strip():
        return legacy.strip()

    # 3) content ä¸å¯ç”¨ â†’ è©¦è‘—å¾ reasoning_content çš„ Draft æŠ½
    rc = msg.get("reasoning_content")
    draft = _extract_draft_from_reasoning(rc or "")
    if draft:
        return draft

    # 4) æœ€å¾Œæ‰çœ‹ç’°å¢ƒé–‹é—œï¼Œæ˜¯å¦æ•´åŒ…ä¸Ÿå›
    if os.getenv("ALLOW_REASONING_FALLBACK", "false").lower() == "true" and isinstance(rc, str) and rc.strip():
        return rc.strip()

    return ""



import re, json

# å¸¸è¦‹çš„ Draft æ¨™è¨˜æ¨£å¼
_DRAFT_PATTERNS = [
    r'Draft[:ï¼š]\s*[\"â€œ](.+?)[\"â€]\s*$',              # Draft: "...."
    r'Draft[:ï¼š]\s*```(?:\w+)?\n(.+?)\n```',          # Draft: ``` ... ```
    r'###\s*Draft\s*\n(.+)$',                         # Markdown æ¨™é¡Œ Draft
    r'è‰ç¨¿[:ï¼š]\s*(.+)$',                              # ä¸­æ–‡ã€Œè‰ç¨¿:ã€
    r'æœ€çµ‚ç¨¿[:ï¼š]\s*(.+)$',                            # ä¸­æ–‡ã€Œæœ€çµ‚ç¨¿:ã€
]

def _extract_draft_from_reasoning(reasoning: str) -> str:
    if not reasoning:
        return ""
    text = reasoning.strip()

    # 1) å…ˆå˜—è©¦åš´æ ¼çš„ Draft æ¨™è¨˜
    for pat in _DRAFT_PATTERNS:
        m = re.search(pat, text, flags=re.S)
        if m and m.group(1):
            draft = m.group(1).strip()
            # å»æ‰å¾Œé¢å¯èƒ½æ¥çš„ã€Œå­—æ•¸çµ±è¨ˆ/Count:ã€
            draft = re.split(r'\n(?:Count|å­—|characters)[:ï¼š]', draft)[0].strip()
            return draft

    # 2) æœ‰äº›æœƒè¼¸å‡º JSONï¼ŒæŠŠ "summary" æ”¾åœ¨ reasoning è£¡
    jm = re.search(r'\{.*\}', text, flags=re.S)
    if jm:
        try:
            obj = json.loads(jm.group(0))
            for key in ("summary", "final", "output", "answer"):
                if isinstance(obj.get(key, ""), str) and obj[key].strip():
                    return obj[key].strip()
        except Exception:
            pass

    # 3) é€€è€Œæ±‚å…¶æ¬¡ï¼šæŠ“æœ€å¾Œä¸€æ®µçœ‹èµ·ä¾†åƒå®Œæ•´ä¸­æ–‡å¥å­çš„å…§å®¹
    sent = re.findall(r'[\u4e00-\u9fffï¼Œã€ï¼›ï¼šï¼šã€Œã€ã€ã€ï¼ˆï¼‰()A-Za-z0-9%\- ]+[ã€‚.!?]', text)
    if sent:
        return sent[-1].strip()

    return ""

# ==============================================================================
# --- OpenAI & LLM äº’å‹•æ¨¡çµ„ ---
# ==============================================================================
def call_openai_api(messages, model=OPENAI_COMPLETION_MODEL, max_tokens=4000, temperature=0.7):
    if not OPENAI_API_KEY:
        logging.error("OPENAI_API_KEY is not set.")
        return "æŠ±æ­‰ï¼ŒAPI Key æœªè¨­å®šï¼Œç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json", "ngrok-skip-browser-warning": "true"}
    data = {"model": model, "messages": messages, "max_tokens": max_tokens, "temperature": temperature}
    try:
        response = requests.post(f"{OPENAI_BASE_URL}/v1/chat/completions", headers=headers, json=data, timeout=980)
        response.raise_for_status()
        resp_json = response.json()
#         logging.info(str(resp_json)) 

        content = _extract_assistant_text_from_response(resp_json)
        if (not content or not content.strip()) and os.getenv("ALLOW_REASONING_FALLBACK", "false").lower() == "true":
            content = str(resp_json)
        
        logging.info(f"OpenAI API å‘¼å«æˆåŠŸï¼Œæ¨¡å‹: {model}ï¼Œå›æ‡‰é•·åº¦: {len(content)}")
        return content
    except requests.exceptions.Timeout:
        logging.error(f"OpenAI API request timed out. Model: {model}")
        return f"æŠ±æ­‰ï¼Œè«‹æ±‚ OpenAI ({model}) æœå‹™è¶…æ™‚ã€‚"
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI API request error: {e}. Model: {model}")
        return f"æŠ±æ­‰ï¼Œé€£æ¥ OpenAI ({model}) æœå‹™æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    except (KeyError, IndexError, TypeError) as e:
        response_text = response.text if 'response' in locals() else 'N/A'
        logging.error(f"OpenAI API response format error: {e} - Response: {response_text}")
        return f"æŠ±æ­‰ï¼ŒOpenAI ({model}) å›æ‡‰æ ¼å¼æœ‰å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unexpected error in call_openai_api: {e}", exc_info=True)
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›è¦†æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ã€‚"

def generate_chat_response(context_id, prompt_text):
    system_prompt = (
        "ä½ æ˜¯ä¸€å€‹åœ¨ Line ç¾¤çµ„æˆ–ç§èŠä¸­çš„èŠå¤©æ©Ÿå™¨äººã€‚ä½ çš„å›ç­”è¦ç²¾ç°¡ã€å£èªåŒ–ï¼Œä½¿ç”¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡ã€‚"
        "ä½ æœƒæ”¶åˆ°ä¸€æ®µåŒ…å«å¤šäººå°è©±çš„æ­·å²ç´€éŒ„ï¼Œæ¯å¥è©±å‰é¢å¯èƒ½æœƒæ¨™ç¤ºç™¼è¨€è€…ã€‚è«‹å®Œå®Œå…¨å…¨æ ¹æ“šå®Œæ•´çš„ä¸Šä¸‹æ–‡é€²è¡Œå›ç­”ã€‚"
        "è«‹æ ¹æ“šæˆ‘å€‘çš„å°è©±æ­·å²ä¾†å›æ‡‰æ‰€æœ‰å•é¡Œã€‚å¿½ç•¥ä»»ä½•å¤–éƒ¨çŸ¥è­˜æˆ–æ–°ä¸»é¡Œï¼Œä¹Ÿä¸è¦æ ¹æ“šå·²çŸ¥è¨˜æ†¶ï¼Œåªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å…§å®¹ç”Ÿæˆç­”æ¡ˆã€‚"
        "å¦‚æœç­”æ¡ˆéœ€è¦æ€è€ƒæ­¥é©Ÿï¼Œè«‹å°‡æ€è€ƒéç¨‹ç”¨ <think> å’Œ </think> æ¨™ç±¤åŒ…èµ·ä¾†ã€‚"
    )
    context_history = CONVERSATION_HISTORY.get(context_id, [])
    messages_for_api = [{"role": "system", "content": system_prompt}] + context_history
    bot_response = call_openai_api(messages_for_api)
    return bot_response

# ==============================================================================
# --- æ–°èæ‘˜è¦èˆ‡æ•´åˆæ¨¡çµ„ ---
# ==============================================================================
def summarize_news_flow(articles_data):
    if not articles_data: return "ä»Šå¤©æ²’æœ‰æŠ“å–åˆ°ç›¸é—œæ–°èå¯ä¾›æ‘˜è¦ã€‚"
    logging.info("--- é–‹å§‹ç¬¬ä¸€éšæ®µæ‘˜è¦ï¼šé€ç¯‡ç²¾ç°¡ ---")
    individual_summaries = []
    for i, article in enumerate(articles_data):
        logging.info(f"  æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(articles_data)} ç¯‡: {article['title']}")
        content_to_summarize = article['text'][:8000]
        user_prompt = f"æ–°èæ¨™é¡Œï¼š{article['title']}\n\næ–°èå…§æ–‡ï¼š\n{content_to_summarize}"
        raw_summary = call_openai_api([{"role": "system", "content": PROMPT_FOR_INDIVIDUAL_SUMMARY}, {"role": "user", "content": user_prompt}], model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini"), max_tokens=3500, temperature=0.2)
        
        logging.info(f"  user_prompt: {user_prompt}")
        logging.info(f"  raw_summary: {raw_summary}")
        
        if raw_summary.startswith("æŠ±æ­‰ï¼Œ"):
            logging.warning(f"  [è·³é] ç¬¬ {i+1} ç¯‡æ–°èæ‘˜è¦å¤±æ•—: {raw_summary}")
            continue
        think_pattern = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)
        cleaned_summary = re.sub(think_pattern, '', raw_summary).strip()
        if len(raw_summary) != len(cleaned_summary): logging.info(f"  å·²æ¸…ç†æ‰ <think> æ¨™ç±¤ã€‚")
        individual_summaries.append({'title': article['title'], 'url': article['url'], 'summary': cleaned_summary,
            'publish_date': article.get('publish_date')})
        logging.info(f"  æ‘˜è¦å®Œæˆï¼Œé•·åº¦: {len(cleaned_summary)} å­—")
        logging.info(f"  ç­‰å¾…30ç§’ é¿å…LLMé€Ÿç‡é™åˆ¶")
        time.sleep(30) # é™ä½LLMé€Ÿç‡
    if not individual_summaries: return "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆéç¨‹ç™¼ç”Ÿå•é¡Œï¼Œç„¡æ³•ç”¢å‡ºå…§å®¹ã€‚"
    logging.info("--- é–‹å§‹ç¬¬äºŒéšæ®µæ‘˜è¦ï¼šå½™æ•´ç”Ÿæˆ Podcast å…§å®¹ ---")
#     summaries_for_prompt = [f"æ–°è {i+1}:\næ¨™é¡Œ: {item['title']}\næ‘˜è¦å…§å®¹: {item['summary']}\n---" for i, item in enumerate(individual_summaries)]
    summaries_for_prompt = []
    for i, item in enumerate(individual_summaries):
        # æ ¼å¼åŒ–æ—¥æœŸï¼Œå¦‚æœä¸å­˜åœ¨å‰‡çµ¦äºˆæç¤º
        date_str = item['publish_date'].strftime("%Y-%m-%d") if item.get('publish_date') else "æ—¥æœŸæœªçŸ¥"
        prompt_line = (
            f"æ–°è {i+1} (ç™¼å¸ƒæ–¼: {date_str}):\n"
            f"æ¨™é¡Œ: {item['title']}\n"
            f"æ‘˜è¦å…§å®¹: {item['summary']}\n---"
        )
        summaries_for_prompt.append(prompt_line)
    
    final_user_prompt = "\n".join(summaries_for_prompt)
    final_summary = call_openai_api([{"role": "system", "content": PROMPT_FOR_FINAL_AGGREGATION}, {"role": "user", "content": final_user_prompt}], model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o"), max_tokens=3000, temperature=0.7)
    return final_summary

# ==============================================================================
# --- Line Bot åŸºç¤åŠŸèƒ½èˆ‡è³‡æ–™è™•ç† ---
# ==============================================================================
def load_json_data(file_path):
    try:
        with open(file_path, "r", encoding='utf-8') as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return {}

def save_json_data(data, file_path):
    try:
        with open(file_path, "w", encoding='utf-8') as f: json.dump(data, f, ensure_ascii=False, indent=4)
    except Exception as e: logging.error(f"å„²å­˜æª”æ¡ˆ {file_path} å¤±æ•—: {e}")

USER_PREFERENCES = load_json_data(USER_PREFERENCES_FILE)
CONVERSATION_HISTORY = load_json_data(CONVERSATION_HISTORY_FILE)
NEWS_CACHE = load_json_data(NEWS_CACHE_FILE) 

def validate_signature(request_body_bytes, signature_header):
    if not LINE_CHANNEL_SECRET: return True
    hash_obj = hmac.new(LINE_CHANNEL_SECRET.encode('utf-8'), request_body_bytes, hashlib.sha256)
    generated_signature = base64.b64encode(hash_obj.digest()).decode('utf-8')
    return hmac.compare_digest(generated_signature, signature_header)

def _utf16_len(s: str) -> int:
    return len(s.encode('utf-16-le')) // 2

def _slice_by_utf16(s: str, max_units: int):
    buf, acc = [], 0
    for ch in s:
        u = _utf16_len(ch)
        if acc + u > max_units:
            yield ''.join(buf)
            buf, acc = [ch], u
        else:
            buf.append(ch); acc += u
    if buf: yield ''.join(buf)
    
def split_long_message(text, limit=None):
    if not text or not text.strip():
        return []
    limit = limit or 5000

    if _utf16_len(text) <= limit:
        return [text.strip()]
    messages = []
    current = ""
    for para in text.split('\n'):
        if _utf16_len(current + para + '\n') <= limit:
            current += para + '\n'
        else:
            if current:
                messages.append(current.strip()); current = ""
            if _utf16_len(para) > limit:
                messages.extend(list(_slice_by_utf16(para, limit)))
            else:
                current = para + '\n'
    if current:
        messages.append(current.strip())

    if len(messages) > 1:
        messages = [f"({i+1}/{len(messages)})\n{m}" for i, m in enumerate(messages)]
    return messages

LAST_PUSH_TS = 0
MIN_PUSH_INTERVAL_SEC = float(os.getenv("LINE_MIN_PUSH_INTERVAL_SEC", "1.2"))

def _throttle():
    global LAST_PUSH_TS
    now = time.time()
    gap = now - LAST_PUSH_TS
    if gap < MIN_PUSH_INTERVAL_SEC:
        time.sleep(MIN_PUSH_INTERVAL_SEC - gap)
    LAST_PUSH_TS = time.time()

def send_line_messages(context_id, reply_token_or_none, text_messages_list):
    if not text_messages_list: return
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}", "Content-Type": "application/json"}

    def _push_one(msg_text):
        _throttle()
        payload = {"to": context_id, "messages": [{"type": "text", "text": str(msg_text)}]}
        r = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
        if r.status_code == 429:
            logging.warning("Push 429ï¼Œå°‡å»¶é²é‡è©¦ä¸€æ¬¡...")
            time.sleep(MIN_PUSH_INTERVAL_SEC * 2.5)
            _throttle()
            r = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
        r.raise_for_status()

    is_first_replied = False
    if reply_token_or_none:
        try:
            payload = {"replyToken": reply_token_or_none, "messages": [{"type": "text", "text": str(text_messages_list[0])}]}
            r = requests.post("https://api.line.me/v2/bot/message/reply", headers=headers, json=payload, timeout=20)
            r.raise_for_status()
            is_first_replied = True
        except requests.exceptions.RequestException as e:
            logging.error(f"Reply å¤±æ•—ï¼Œæ”¹ç”¨ Pushï¼š{e}")

    start = 1 if is_first_replied else 0
    for i in range(start, len(text_messages_list)):
        try:
            _push_one(text_messages_list[i])
        except requests.exceptions.RequestException as e:
            logging.error(f"Push å¤±æ•— part {i+1}ï¼š{e}")
            break
        
def get_user_profile(context_id, user_id):
    cache_key = (context_id, user_id)
    current_time = time.time()
    if cache_key in USER_PROFILE_CACHE and current_time - USER_PROFILE_CACHE[cache_key]['timestamp'] < USER_PROFILE_CACHE_SECONDS:
        return USER_PROFILE_CACHE[cache_key]['displayName']
    if context_id.startswith('G') or context_id.startswith('R'): url = f"https://api.line.me/v2/bot/group/{context_id}/member/{user_id}"
    elif context_id.startswith('U'): url = f"https://api.line.me/v2/bot/profile/{user_id}"
    else: return "æœªçŸ¥ç”¨æˆ¶"
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        profile_data = response.json()
        display_name = profile_data.get("displayName", "ç„¡åæ°")
        USER_PROFILE_CACHE[cache_key] = {"displayName": display_name, "timestamp": current_time}
        logging.info(f"é€é API å–å¾—ç”¨æˆ¶ {user_id} çš„åç¨±: {display_name}ï¼Œä¸¦å·²æ›´æ–°å¿«å–ã€‚")
        return display_name
    except requests.exceptions.RequestException as e:
        logging.warning(f"ç„¡æ³•ç²å–ç”¨æˆ¶ {user_id} çš„å€‹äººè³‡æ–™: {e}")
        return "æŸä½æˆå“¡"

# ==============================================================================
# --- æ ¸å¿ƒæ¥­å‹™é‚è¼¯èˆ‡ Webhook äº‹ä»¶è™•ç† ---
# ==============================================================================
def generate_and_push_news_for_user(user_id, user_custom_keywords=None, is_immediate_push=False, reply_token=None):
    log_prefix = "å³æ™‚è«‹æ±‚" if is_immediate_push else "æ’ç¨‹æ¨æ’­"
    logging.info(f"[{log_prefix}] é–‹å§‹ç‚ºç”¨æˆ¶ {user_id} è™•ç†æ–°èè«‹æ±‚...")
    
    theme_name = user_custom_keywords if user_custom_keywords else "é è¨­ AI ä¸»é¡Œ"
    cache_key = user_custom_keywords if user_custom_keywords else "__DEFAULT__"
    current_time = time.time()

    if cache_key in NEWS_CACHE:
        cached_item = NEWS_CACHE[cache_key]
        cache_age = current_time - cached_item.get("timestamp", 0)
        
        if cache_age < NEWS_SUMMARY_CACHE_SECONDS:
            logging.info(f"æ–°èå¿«å–å‘½ä¸­ï¼(é—œéµå­—: '{cache_key}', å¹´é½¡: {int(cache_age)}ç§’)")
            cached_reply_content = cached_item.get("reply_content")
            if cached_reply_content:
                final_reply = f"é€™ä»½æ–°èæ‘˜è¦æ ¹æ“šã€Œ{theme_name}ã€ä¸»é¡Œç”¢ç”Ÿï¼ˆå¾å¿«å–æä¾›ğŸ˜Šï¼‰\n\n{cached_reply_content}"
                send_line_messages(user_id, reply_token, split_long_message(final_reply))
                return

    logging.info(f"æ–°èå¿«å–æœªå‘½ä¸­æˆ–å·²éæœŸ (é—œéµå­—: '{cache_key}')ï¼ŒåŸ·è¡Œå®Œæ•´æ–°èæ‘˜è¦æµç¨‹ã€‚")
    articles = fetch_and_parse_articles(custom_query=user_custom_keywords, limit=NEWS_FETCH_TARGET_COUNT)
    if not articles:
        send_line_messages(user_id, reply_token, [f"æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½æ ¹æ“šæ‚¨çš„é—œéµå­—ã€Œ{theme_name}ã€æ‰¾åˆ°å¯æˆåŠŸæ“·å–çš„æ–°èã€‚"])
        return

    final_summary_raw = summarize_news_flow(articles)
    if not final_summary_raw or final_summary_raw.startswith("æŠ±æ­‰ï¼Œ"):
        send_line_messages(user_id, reply_token, [final_summary_raw or "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆç•°å¸¸ï¼Œå…§å®¹ç‚ºç©ºã€‚"])
        return

    parsed_result = handle_llm_response_with_think(final_summary_raw)
    thinking_messages = parsed_result["thinking_messages"]
    formal_messages = parsed_result["formal_messages"]

    final_formal_reply_for_cache = ""
    if formal_messages:
        generation_time = datetime.fromtimestamp(current_time)
        time_str = generation_time.strftime("%Y-%m-%d %H:%M")
        full_formal_text = "\n".join(formal_messages)
        final_formal_reply_for_cache = f"ç”¢ç”Ÿæ–¼ {time_str}\n\n{full_formal_text}"
    
    if final_formal_reply_for_cache:
        NEWS_CACHE[cache_key] = {
            "timestamp": current_time,
            "reply_content": final_formal_reply_for_cache
        }
        save_json_data(NEWS_CACHE, NEWS_CACHE_FILE)
        logging.info(f"å·²æ›´æ–°æ–°èå¿«å– (é—œéµå­—: '{cache_key}')ã€‚")

    final_reply_for_user = f"é€™ä»½æ–°èæ‘˜è¦æ ¹æ“šã€Œ{theme_name}ã€ä¸»é¡Œç”¢ç”Ÿ\n\n{final_formal_reply_for_cache}"
    
    messages_to_send = thinking_messages + split_long_message(final_reply_for_user)
    send_line_messages(user_id, reply_token, messages_to_send)
    
    logging.info(f"[{log_prefix}] å·²å®Œæˆå°ç”¨æˆ¶ {user_id} çš„æ–°èæ¨é€ã€‚")

def generate_news_for_single_user_job(user_id, keywords, remaining_users, is_immediate=False):
    with app.app_context():
        log_prefix = "èƒŒæ™¯å³æ™‚è«‹æ±‚" if is_immediate else "èƒŒæ™¯æ’ç¨‹æ¨æ’­"
        logging.info(f"[{log_prefix}] ä»»å‹™éˆå•Ÿå‹•ï¼Œç‚ºç”¨æˆ¶ {user_id} ç”¢ç”Ÿæ–°è...")
        try:
            generate_and_push_news_for_user(user_id=user_id, user_custom_keywords=keywords, is_immediate_push=is_immediate, reply_token=None)
        except Exception as e:
            logging.error(f"[{log_prefix}] èƒŒæ™¯ä»»å‹™ç‚ºç”¨æˆ¶ {user_id} ç”¢ç”Ÿæ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        finally:
            if remaining_users:
                next_user_id, next_user_keywords = remaining_users[0]
                next_remaining_users = remaining_users[1:]
                run_time = datetime.now(scheduler.timezone) + timedelta(seconds=10)
                job_id = f"scheduled_chain_{next_user_id}_{int(run_time.timestamp())}"
                scheduler.add_job(generate_news_for_single_user_job, 'date', run_date=run_time, args=[next_user_id, next_user_keywords, next_remaining_users, False], id=job_id)
                logging.info(f"ä»»å‹™éˆï¼šç‚ºç”¨æˆ¶ {user_id} çš„ä»»å‹™å·²å®Œæˆï¼Œå·²è¨»å†Šä¸‹ä¸€å€‹ä»»å‹™çµ¦ {next_user_id}ã€‚")
            else:
                logging.info(f"ä»»å‹™éˆï¼šç‚ºç”¨æˆ¶ {user_id} çš„ä»»å‹™å·²å®Œæˆï¼Œä»»å‹™éˆçµæŸã€‚")

@app.route('/webhook', methods=['POST'])
def webhook():
    signature = request.headers.get("X-Line-Signature")
    body_bytes = request.get_data()
    if not validate_signature(body_bytes, signature):
        logging.error("Webhook: Invalid signature.")
        return jsonify({"status": "invalid signature"}), 400
    try:
        data = request.json
        for event in data.get("events", []):
            source, event_type, reply_token = event.get("source", {}), event.get("type"), event.get("replyToken")
            source_type = source.get("type")
            context_id = source.get(f'{source_type}Id') if source_type else None
            if not context_id: continue
            logging.info(f"æ”¶åˆ°äº‹ä»¶: type={event_type}, source_type={source_type}, context_id={context_id}")
            if event_type == "message" and event.get("message", {}).get("type") == "text":
                handle_text_message_event(context_id=context_id, user_id=source.get('userId'), reply_token=reply_token, user_text=event["message"]["text"])
            elif event_type == "follow":
                user_pref = USER_PREFERENCES.get(context_id, {})
                user_pref["subscribed_news"] = True
                USER_PREFERENCES[context_id] = user_pref
                save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
                send_line_messages(context_id, reply_token, ["æ„Ÿè¬æ‚¨åŠ æˆ‘å¥½å‹ï¼è¼¸å…¥ `/bot å¹«åŠ©` å¯ä»¥æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤å–”ã€‚"])
            elif event_type == "unfollow" and context_id in USER_PREFERENCES:
                USER_PREFERENCES[context_id]["subscribed_news"] = False
                save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
        return jsonify({"status": "success"}), 200
    except Exception as e:
        logging.error(f"è™•ç† webhook æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500

def handle_llm_response_with_think(llm_full_response):
    think_pattern = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
    result = {"thinking_messages": [], "formal_messages": []}
    show_thinking = os.getenv("SHOW_THINKING_PROCESS", "false").lower() == "true"
    fallback_on_empty = os.getenv("FALLBACK_ON_EMPTY", "true").lower() == "true"
    match = think_pattern.search(llm_full_response or "")
    if match:
        thinking_text = (match.group(1) or "").strip()
        formal_text = (llm_full_response[match.end():] or "").strip()
        if thinking_text and show_thinking:
            result["thinking_messages"] = split_long_message(f"âš™ï¸ æˆ‘çš„æ€è€ƒéç¨‹ï¼š\n{thinking_text}")
        if formal_text:
            result["formal_messages"] = split_long_message(formal_text)
        else:
            if fallback_on_empty:
                cleaned = think_pattern.sub("", llm_full_response or "").strip()
                if cleaned:
                    result["formal_messages"] = split_long_message(cleaned)
                else:
                    raw = (llm_full_response or "").strip()
                    if raw:
                        result["formal_messages"] = split_long_message(raw)
    else:
        cleaned = (llm_full_response or "").strip()
        if cleaned:
            result["formal_messages"] = split_long_message(cleaned)
        elif fallback_on_empty:
            result["formal_messages"] = split_long_message(llm_full_response or "")
    return result

def handle_text_message_event(context_id, user_id, reply_token, user_text):
    display_name = get_user_profile(context_id, user_id)
    if context_id.startswith(('G', 'R')): formatted_message_content = f"{display_name}: {user_text}"
    else: formatted_message_content = user_text
    history = CONVERSATION_HISTORY.get(context_id, [])
    history.append({"role": "user", "content": formatted_message_content})
    if len(history) > MAX_HISTORY_MESSAGES: history = history[-MAX_HISTORY_MESSAGES:]
    CONVERSATION_HISTORY[context_id] = history
    save_json_data(CONVERSATION_HISTORY, CONVERSATION_HISTORY_FILE)
    logging.info(f"å·²è¨˜éŒ„è¨Šæ¯åˆ° {context_id}ã€‚ç•¶å‰æ­·å²é•·åº¦: {len(history)}")

    user_text_stripped = user_text.strip()
    if not user_text_stripped.startswith(BOT_TRIGGER_WORD): return

    command_text = user_text_stripped[len(BOT_TRIGGER_WORD):].strip()
    if not command_text or command_text.lower() in ["help", "å¹«åŠ©", "æŒ‡ä»¤"]:
        send_line_messages(context_id, reply_token, [HELP_MESSAGE.strip()]); return

    cmd_parts = command_text.lower().split()
    main_command = cmd_parts[0] if cmd_parts else ""

    if main_command in ["æ–°è", "news", "æ–°èæ‘˜è¦"]:
        logging.info("åµæ¸¬åˆ°ã€Œæ–°èä¸€æ¬¡æ€§æŸ¥è©¢ã€æŒ‡ä»¤ã€‚")
        final_keywords = None; user_input_part = command_text[len(main_command):].strip()
        if user_input_part:
            if user_input_part.lower().startswith("é—œéµå­—:"): final_keywords = user_input_part[len("é—œéµå­—:"):].strip()
            else: final_keywords = user_input_part
        else:
            user_pref = USER_PREFERENCES.get(context_id, {});
            if user_pref.get("subscribed_news") and user_pref.get("news_keywords"): final_keywords = user_pref.get("news_keywords")
        if not final_keywords: final_keywords = None
        # ä½¿ç”¨èƒŒæ™¯åŸ·è¡Œç·’è™•ç†ï¼Œé¿å… webhook è¶…æ™‚
        thread = ThreadPoolExecutor(max_workers=1)
        thread.submit(generate_and_push_news_for_user, user_id=context_id, user_custom_keywords=final_keywords, is_immediate_push=True, reply_token=reply_token)
        # ç«‹åˆ»å›è¦†ä¸€å€‹è™•ç†ä¸­è¨Šæ¯
        send_line_messages(context_id, reply_token, ["æ”¶åˆ°ï¼æ­£åœ¨ç‚ºæ‚¨å®¢è£½åŒ–æ–°èæ‘˜è¦ï¼Œè«‹ç¨å€™... ğŸš€"])


    elif main_command == "è¨‚é–±":
        logging.info("åµæ¸¬åˆ°ã€Œè¨‚é–±ã€æŒ‡ä»¤ã€‚")
        keywords_to_subscribe = command_text[len(main_command):].strip()
        user_pref = USER_PREFERENCES.get(context_id, {}); user_pref["subscribed_news"] = True; user_pref["news_keywords"] = keywords_to_subscribe or None
        reply_msg = f"âœ… è¨­å®šæˆåŠŸï¼å·²ç‚ºæ‚¨è¨‚é–±æ¯æ—¥æ–°èï¼Œä¸»é¡Œç‚ºï¼šã€Œ{keywords_to_subscribe or 'é è¨­ AI ä¸»é¡Œ'}ã€ã€‚"
        USER_PREFERENCES[context_id] = user_pref; save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE)
        send_line_messages(context_id, reply_token, [reply_msg])

    elif main_command == "æŸ¥çœ‹è¨‚é–±":
        user_pref = USER_PREFERENCES.get(context_id, {}); reply_msg = "æ‚¨ç›®å‰å°šæœªè¨‚é–±æ¯æ—¥æ–°èå–”ã€‚"
        if user_pref.get("subscribed_news"): subscribed_keywords = user_pref.get("news_keywords", "é è¨­ AI ä¸»é¡Œ"); reply_msg = f"æ‚¨ç›®å‰çš„è¨‚é–±ç‹€æ…‹ç‚ºï¼š\n- ç‹€æ…‹ï¼šå·²è¨‚é–± âœ…\n- ä¸»é¡Œï¼šã€Œ{subscribed_keywords}ã€"
        send_line_messages(context_id, reply_token, [reply_msg])

    elif main_command == "å–æ¶ˆè¨‚é–±":
        user_pref = USER_PREFERENCES.get(context_id, {}); user_pref["subscribed_news"] = False; USER_PREFERENCES[context_id] = user_pref
        save_json_data(USER_PREFERENCES, USER_PREFERENCES_FILE); send_line_messages(context_id, reply_token, ["â˜‘ï¸ å¥½çš„ï¼Œå·²ç‚ºæ‚¨å–æ¶ˆæ¯æ—¥æ–°èè¨‚é–±ã€‚"])
        
    else:
        logging.info("ä½œç‚ºä¸€èˆ¬èŠå¤©å•é¡Œè™•ç†ã€‚")
        llm_response = generate_chat_response(context_id, command_text)
        
        parsed_result = handle_llm_response_with_think(llm_response)
        thinking_messages = parsed_result["thinking_messages"]
        formal_messages = parsed_result["formal_messages"]
        messages_to_send = thinking_messages + formal_messages
        send_line_messages(context_id, reply_token, messages_to_send)
        
        if not llm_response.startswith("æŠ±æ­‰ï¼Œ"):
            cleaned_bot_response = "\n".join(formal_messages)
            history.append({"role": "assistant", "content": cleaned_bot_response})
            if len(history) > MAX_HISTORY_MESSAGES:
                history = history[-MAX_HISTORY_MESSAGES:]
            CONVERSATION_HISTORY[context_id] = history
            save_json_data(CONVERSATION_HISTORY, CONVERSATION_HISTORY_FILE)       

# ==============================================================================
# --- æ’ç¨‹èˆ‡æ‡‰ç”¨å•Ÿå‹• ---
# ==============================================================================
scheduler = BackgroundScheduler(timezone="Asia/Taipei", daemon=True)

def daily_news_push_job():
    with app.app_context():
        logging.info("APScheduler: ä»»å‹™éˆå•Ÿå‹•å™¨é–‹å§‹åŸ·è¡Œ...")
        users_to_push = [(uid, prefs.get("news_keywords")) for uid, prefs in load_json_data(USER_PREFERENCES_FILE).items() if prefs.get("subscribed_news")]
        if TARGET_USER_ID_FOR_TESTING and not any(u[0] == TARGET_USER_ID_FOR_TESTING for u in users_to_push):
            users_to_push.append((TARGET_USER_ID_FOR_TESTING, load_json_data(USER_PREFERENCES_FILE).get(TARGET_USER_ID_FOR_TESTING, {}).get("news_keywords")))
        if not users_to_push:
            logging.info("APScheduler: å•Ÿå‹•å™¨ç™¼ç¾æ²’æœ‰éœ€è¦è™•ç†çš„ç”¨æˆ¶ã€‚")
            return
        logging.info(f"APScheduler: å•Ÿå‹•å™¨æº–å‚™å•Ÿå‹•ä¸€å€‹åŒ…å« {len(users_to_push)} ä½ç”¨æˆ¶çš„ä»»å‹™éˆã€‚")
        first_user_id, first_user_keywords = users_to_push[0]
        remaining_users = users_to_push[1:]
        job_id = f"scheduled_chain_{first_user_id}_{int(time.time())}"
        scheduler.add_job(generate_news_for_single_user_job, 'date', run_date=datetime.now(scheduler.timezone) + timedelta(seconds=5), args=[first_user_id, first_user_keywords, remaining_users, False], id=job_id)
        logging.info(f"APScheduler: ä»»å‹™éˆçš„ç¬¬ä¸€å€‹ä»»å‹™å·²è¨»å†Šçµ¦ {first_user_id}ï¼Œå•Ÿå‹•å™¨ä»»å‹™çµæŸã€‚")

def shutdown_scheduler_on_exit():
    if scheduler.running: scheduler.shutdown(wait=False)

def _debug_test_call_openai_api():
    try:
        base = os.getenv("OPENAI_BASE_URL", "https://api.openai.com")
        model = os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini")
        key = os.getenv("OPENAI_API_KEY")
        print("[TEST] OPENAI_BASE_URL =", base)
        print("[TEST] OPENAI_COMPLETION_MODEL =", model)
        print("[TEST] OPENAI_API_KEY set? ", "YES" if key else "NO")
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ç°¡æ½”åŠ©ç†ï¼Œå›è¦†ä¸è¶…é20å­—ã€‚"},
            {"role": "user", "content": "å›è¦†ï¼šOK å³è¡¨ç¤ºAPIå¯ç”¨ã€‚"}
        ]
        out = call_openai_api(messages, model=model, max_tokens=1164, temperature=0.0)
        if out is None:
            print("[TEST] call_openai_api å›å‚³ None")
        else:
            print("[TEST] å›å‚³é•·åº¦ =", len(out))
            print("[TEST] å…§å®¹å‰1300å­—ï¼š", (out or "")[:1300])
        if not out or not str(out).strip():
            print("[TEST][WARN] å›æ‡‰ç‚ºç©ºå­—ä¸²ï¼å¯èƒ½æ˜¯ proxy / base_url / body æ ¼å¼ / æ¨¡å‹åéŒ¯èª¤ã€‚")
    except Exception as e:
        print("[TEST][ERROR] ä¾‹å¤–ï¼š", repr(e))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Line Bot and News Fetcher")
    parser.add_argument('--test-news', action='store_true', help='Run in local test mode for news fetching and summarization.')
    parser.add_argument('--test-openai', action='store_true', help='Quickly test call_openai_api pipeline.')
    parser.add_argument('--keywords', type=str, default=None, help='Keywords for news fetching in test mode.')
    parser.add_argument('--limit', type=int, default=None, help='Number of articles to process in test mode.')
    args = parser.parse_args()

    if args.test_openai:
        _debug_test_call_openai_api()
        sys.exit(0)

    if args.test_news:
        def run_test_mode(keywords, limit):
            print("="*50 + "\nğŸš€ é€²å…¥æœ¬åœ°æ¸¬è©¦æ¨¡å¼ ğŸš€\n" + "="*50)
            articles = fetch_and_parse_articles(custom_query=keywords, limit=limit or NEWS_FETCH_TARGET_COUNT)
            if not articles:
                print("[!] æ¸¬è©¦ä¸­æ­¢ï¼šæœªèƒ½æˆåŠŸæ“·å–ä»»ä½•æ–°èå…§æ–‡ã€‚")
                return
            final_summary = summarize_news_flow(articles)
            print("\n" + "="*50 + "\nğŸ§ æœ€çµ‚ Podcast é¢¨æ ¼æ‘˜è¦ ğŸ§\n" + "="*50)
            print(final_summary)
            print("\n" + "="*50 + "\nâœ… æ¸¬è©¦æµç¨‹çµæŸ âœ…\n" + "="*50)
        run_test_mode(args.keywords, args.limit)
    else:
        logging.info("ğŸš€ å•Ÿå‹• Flask Web ä¼ºæœå™¨æ¨¡å¼ ğŸš€")
        required_env_vars = ['LINE_CHANNEL_ACCESS_TOKEN', 'LINE_CHANNEL_SECRET', 'OPENAI_API_KEY']
        if any(not os.getenv(var) for var in required_env_vars):
            logging.critical(f"CRITICAL: Missing required environment variables: {', '.join(v for v in required_env_vars if not os.getenv(v))}. Exiting.")
            exit(1)
        if not scheduler.get_jobs():
            scheduler.add_job(daily_news_push_job, 'cron', hour=9, minute=0, id='daily_news_cron_morning', replace_existing=True)
            scheduler.add_job(daily_news_push_job, 'cron', hour=16, minute=0, id='daily_news_cron_afternoon', replace_existing=True)
            logging.info("å·²è¨­å®šæ¯æ—¥ 09:00 å’Œæ¯æ—¥ 16:00 çš„æ–°èæ¨æ’­æ’ç¨‹ã€‚")
            if os.getenv("RUN_JOB_ON_STARTUP", "False").lower() == "true":
                scheduler.add_job(daily_news_push_job, 'date', run_date=datetime.now(scheduler.timezone) + timedelta(seconds=15), id='startup_news_push')
                logging.info(f"å·²è¨­å®šåœ¨ 15 ç§’å¾ŒåŸ·è¡Œä¸€æ¬¡æ–°èæ¨æ’­ä»»å‹™ã€‚")
        if not scheduler.running:
            scheduler.start()
            logging.info("APScheduler started.")
            atexit.register(shutdown_scheduler_on_exit)
        port = int(os.environ.get("PORT", 5000))
        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)
