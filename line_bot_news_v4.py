# ==============================================================================
# line_bot_final_merged.py
# æ•´åˆäº† Line Botã€Selenium æ–°èæ“·å–ã€å…©éšæ®µ LLM æ‘˜è¦åŠæœ¬åœ°æ¸¬è©¦æ¨¡å¼çš„å®Œæ•´ç¨‹å¼ç¢¼
# ==============================================================================

# --- Python Standard Libraries ---
import os
import platform 
import sys
import time
import json
import logging
import hashlib
import hmac
import base64
import re
import atexit
import argparse
from datetime import datetime, timedelta
import urllib.parse

# --- Third-party Libraries ---
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from apscheduler.schedulers.background import BackgroundScheduler
import feedparser

# --- Newspaper3k for article scraping ---
from newspaper import Article, Config

# --- Selenium for dynamic content scraping ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ==============================================================================
# --- ç’°å¢ƒè¨­å®šã€æ—¥èªŒèˆ‡ Flask åˆå§‹åŒ– ---
# ==============================================================================
load_dotenv()
app = Flask(__name__)

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]',
    stream=sys.stdout
)

# --- å…¨åŸŸè®Šæ•¸èˆ‡å¸¸æ•¸ ---
BOT_NAMES = os.getenv("BOT_NAMES", "bot,æ©Ÿå™¨äºº").split(",")
BOT_DEACTIVATED = os.getenv("BOT_DEACTIVATED", "False").lower() == "true"
OPENAI_COMPLETION_MODEL = os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com")
TARGET_USER_ID_FOR_TESTING = os.getenv("TARGET_USER_ID_FOR_TESTING")

VISUAL_SEPARATION_DELAY = float(os.getenv("VISUAL_SEPARATION_DELAY", "1.0"))
DEFAULT_NEWS_KEYWORDS = "å¤§å‹èªè¨€æ¨¡å‹ OR LLM OR ç”Ÿæˆå¼AI OR OpenAI OR Gemini OR Claude"
USER_PREFERENCES_FILE = "user_preferences.json"
MAX_MESSAGE_LENGTH = 4800
NEWS_FETCH_TARGET_COUNT = 7 # å¾ RSS ä¸­å˜—è©¦æŠ“å–çš„ç›®æ¨™æ–°èæ•¸é‡
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'

# --- å…©éšæ®µæ‘˜è¦çš„ LLM Prompt è¨­å®š ---
PROMPT_FOR_INDIVIDUAL_SUMMARY = (
    "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æ–°èç·¨è¼¯ï¼Œå°ˆé•·æ˜¯å¿«é€Ÿæç…‰æ–‡ç« æ ¸å¿ƒã€‚è«‹å°‡ä»¥ä¸‹æä¾›çš„æ–°èå…§æ–‡ï¼Œæ¿ƒç¸®æˆä¸€æ®µä¸è¶…é150å­—çš„å®¢è§€ã€ç²¾ç°¡ä¸­æ–‡æ‘˜è¦ã€‚"
    "æ‘˜è¦æ‡‰åŒ…å«æœ€é—œéµçš„äººç‰©ã€äº‹ä»¶ã€æ•¸æ“šå’Œçµè«–ã€‚è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹é ­æˆ–çµå°¾çš„å®¢å¥—è©±ã€‚"
)

PROMPT_FOR_FINAL_AGGREGATION = (
    "ä½ æ˜¯ä¸€ä½é¢¨è¶£å¹½é»˜ã€çŸ¥è­˜æ·µåšçš„ç§‘æŠ€æ–°è Podcast ä¸»æŒäººã€‚ä½ çš„è½çœ¾æ˜¯ Line ç”¨æˆ¶ï¼Œä»–å€‘å–œæ­¡è¼•é¬†ã€æ˜“æ‡‚ä¸”å¸¶æœ‰ Emoji çš„å…§å®¹ã€‚"
    "æ¥ä¸‹ä¾†æˆ‘æœƒæä¾›æ•¸å‰‡ã€Œå·²ç¶“è¢«ç²¾ç°¡éçš„æ–°èæ‘˜è¦ã€ã€‚è«‹ä½ æ ¹æ“šé€™äº›æ‘˜è¦ï¼Œç™¼æ®ä½ çš„ä¸»æŒé¢¨æ ¼ï¼Œå°‡å®ƒå€‘æ•´åˆæˆä¸€ç¯‡é€£è²«çš„è«‡è©±æ€§å…§å®¹ã€‚"
    "ä½ çš„ä»»å‹™æ˜¯ï¼š\n"
    "1. ç”¨ç”Ÿå‹•çš„èªæ°£é–‹å ´ï¼Œå¸å¼•è½çœ¾æ³¨æ„ã€‚\n"
    "2. å°‡å„å‰‡æ–°èæ‘˜è¦è‡ªç„¶åœ°ä¸²é€£èµ·ä¾†ï¼Œå¯ä»¥åŠ ä¸Šä½ çš„è©•è«–æˆ–è§€é»ä¾†éŠœæ¥ï¼Œä½†ä¸è¦æœæ’°ä¸å­˜åœ¨çš„äº‹å¯¦ã€‚\n"
    "3. åœ¨æåˆ°æ¯å‰‡æ–°èçš„é‡é»å¾Œï¼Œè«‹å‹™å¿…é™„ä¸Šé€™å‰‡æ–°èçš„åŸå§‹æ¨™é¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
    "   - æ¨™é¡Œï¼š[åŸå§‹æ–°èæ¨™é¡Œ]\n"
    "4. å…¨ç¨‹å¤šä½¿ç”¨ Emoji ä¾†å¢åŠ æ´»æ½‘æ„Ÿã€‚\n"
    "5. æœ€å¾Œç”¨ä¸€å¥è©±åšå€‹ç¸½çµæˆ–çµ¦è½çœ¾ä¸€å¥æº«é¦¨æé†’ã€‚\n"
    "6. åš´æ ¼æ ¹æ“šæˆ‘æä¾›çš„æ‘˜è¦å…§å®¹ã€æ¨™é¡Œå’Œé€£çµé€²è¡Œå‰µä½œï¼Œä¸è¦å¼•ç”¨å¤–éƒ¨è³‡è¨Šã€‚\n"
    "7. è¦åš´è‚…æ‡‰å°æ¯å‰‡æ–°èçš„è² é¢æƒ…ç·’ã€‚\n"
    "8. æœ€å¾Œçµè«–è¦åŠ è¨»é€™æ˜¯AIè² è²¬ç¸½çµçš„å…§å®¹ï¼Œè®€è€…æ‡‰è‡ªè¡Œæ±‚è­‰å…¶æ­£ç¢ºæ€§ã€‚\n"
)


# ==============================================================================
# --- æ–°èæ“·å–æ¨¡çµ„ (æ•´åˆè‡ª news_fetch.py) ---
# ==============================================================================
newspaper_config = Config()
newspaper_config.browser_user_agent = USER_AGENT
newspaper_config.request_timeout = 15
newspaper_config.memoize_articles = False

chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1024x400")
chrome_options.add_argument(f"user-agent={USER_AGENT}")

def get_real_url(google_news_url):
    try:
        response = requests.head(google_news_url, allow_redirects=True, timeout=15)
        return response.url
    except requests.RequestException as e:
        logging.warning(f"    [éŒ¯èª¤] ç„¡æ³•è§£æè·³è½‰é€£çµ {google_news_url}: {e}")
        return None

def fetch_article_with_selenium(url):
    logging.info(f"    [å‚™æ´] å˜—è©¦ä½¿ç”¨ Selenium æŠ“å–å‹•æ…‹å…§å®¹: {url[:70]}...")
    driver = None
    try:
        # ==================== æ™ºæ…§åˆ¤æ–·ä½œæ¥­ç³»çµ±ä¸¦è¨­å®š Service ====================
        current_os = platform.system()
        logging.info(f"    [Selenium è¨­å®š] åµæ¸¬åˆ°ç›®å‰ä½œæ¥­ç³»çµ±ç‚º: {current_os}")

        if current_os == "Linux":
            # åœ¨ Linux ç’°å¢ƒ (ä¾‹å¦‚æ‚¨çš„ ARM ä¼º-æœå™¨)ï¼Œä½¿ç”¨æ‰‹å‹•æŒ‡å®šè·¯å¾‘
            # é€™å€‹è·¯å¾‘é€šå¸¸æ˜¯é€é `sudo apt-get install chromium-chromedriver` å®‰è£çš„
            chromedriver_path = "/usr/bin/chromedriver"
            logging.info(f"    [Selenium è¨­å®š] Linux ç³»çµ±ï¼Œä½¿ç”¨æ‰‹å‹•æŒ‡å®šçš„è·¯å¾‘: {chromedriver_path}")
            
            # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡çµ¦å‡ºæ¸…æ™°çš„éŒ¯èª¤æç¤º
            if not os.path.exists(chromedriver_path):
                logging.error(f"    [Selenium åš´é‡éŒ¯èª¤] åœ¨ Linux ç³»çµ±ä¸Šæ‰¾ä¸åˆ°æŒ‡å®šçš„ chromedriver: {chromedriver_path}")
                logging.error("    è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ `sudo apt-get install chromium-chromedriver`ï¼Œæˆ–è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
                return None # ä¸­æ­¢æ­¤å‡½æ•¸çš„åŸ·è¡Œ
                
            service = Service(executable_path=chromedriver_path)
            
        else:
            # åœ¨é Linux ç’°å¢ƒ (ä¾‹å¦‚ Windows, macOS)ï¼Œä½¿ç”¨ webdriver-manager è‡ªå‹•ç®¡ç†
            logging.info("    [Selenium è¨­å®š] é Linux ç³»çµ±ï¼Œä½¿ç”¨ webdriver-manager è‡ªå‹•ä¸‹è¼‰/ç®¡ç† driverã€‚")
            service = Service(ChromeDriverManager().install())
        # ======================================================================
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        time.sleep(4)
        return driver.page_source
    except Exception as e:
        logging.error(f"    [Selenium éŒ¯èª¤] æŠ“å– {url[:70]}... å¤±æ•—: {e}")
        return None
    finally:
        if driver:
            driver.quit()

def fetch_and_parse_articles(custom_query=None, limit=NEWS_FETCH_TARGET_COUNT):
    query_to_use = custom_query.strip() if custom_query and custom_query.strip() else DEFAULT_NEWS_KEYWORDS
    encoded_query = urllib.parse.quote_plus(query_to_use)
    rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    logging.info(f">>> é–‹å§‹å¾ Google News RSS å–å¾—æ–°èåˆ—è¡¨ (é—œéµå­—: '{query_to_use}')")
    feed = feedparser.parse(rss_url)

    if feed.bozo:
        logging.error(f"ç„¡æ³•è§£æ RSS feedã€‚éŒ¯èª¤è³‡è¨Š: {feed.bozo_exception}")
        return []

    successful_articles = []
    processed_urls = set()
    
    logging.info(f">>> æ‰¾åˆ° {len(feed.entries)} å‰‡æ–°èï¼Œé–‹å§‹é€ä¸€çˆ¬å–å…§æ–‡ï¼Œç›®æ¨™ {limit} å‰‡...")

    for entry in feed.entries:
        if len(successful_articles) >= limit:
            break

        logging.info(f"--- æ­£åœ¨è™•ç†: {entry.title}")
        
        real_url = get_real_url(entry.link)
        if not real_url or real_url in processed_urls:
            logging.warning("    [è·³é] ç„¡æ³•å–å¾—çœŸå¯¦ URL æˆ– URL é‡è¤‡ã€‚")
            continue
        
        processed_urls.add(real_url)

        try:
            article = Article(real_url, language='zh', config=newspaper_config)
            article.download()
            article.parse()
            
            if len(article.text) < 200:
                logging.warning("    [è­¦å‘Š] æ¨™æº–æ–¹æ³•æŠ“å–å…§å®¹éçŸ­ï¼Œå•Ÿç”¨ Selenium å‚™æ´ã€‚")
                html_content = fetch_article_with_selenium(real_url)
                if html_content:
                    article.download(input_html=html_content)
                    article.parse()

            if article.title and len(article.text) > 200:
                logging.info(f"    [æˆåŠŸ] å·²å–å¾—æ–‡ç« : {article.title}")
                successful_articles.append({
                    'title': article.title,
                    'text': article.text,
                    'url': real_url,
                    'source': entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else "æœªçŸ¥ä¾†æº"
                })
            else:
                logging.warning(f"    [å¤±æ•—] ä½¿ç”¨æ‰€æœ‰æ–¹æ³•å¾Œï¼Œä»ç„¡æ³•è§£æå‡ºè¶³å¤ å…§æ–‡ã€‚URL: {real_url}")

        except Exception as e:
            logging.error(f"    [å¤±æ•—] è™•ç†æ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ã€‚ URL: {real_url}, åŸå› : {e}", exc_info=True)
        finally:
            time.sleep(1)

    logging.info(f">>> æ–°èå…§æ–‡æ“·å–å®Œæˆï¼Œå…±æˆåŠŸå–å¾— {len(successful_articles)} ç¯‡ã€‚")
    return successful_articles


# ==============================================================================
# --- OpenAI & LLM äº’å‹•æ¨¡çµ„ ---
# ==============================================================================

def call_openai_api(system_prompt, user_prompt, model=OPENAI_COMPLETION_MODEL, max_tokens=4000, temperature=0.7):
    if not OPENAI_API_KEY:
        logging.error("OPENAI_API_KEY is not set.")
        return "æŠ±æ­‰ï¼ŒAPI Key æœªè¨­å®šï¼Œç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚"

    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
        "ngrok-skip-browser-warning": "true"
        }
    data = { "model": model, "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], "max_tokens": max_tokens, "temperature": temperature }
    
    try:
        response = requests.post(f"{OPENAI_BASE_URL}/v1/chat/completions", headers=headers, json=data, timeout=980)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"].strip()
        logging.info(f"OpenAI API å‘¼å«æˆåŠŸï¼Œæ¨¡å‹: {model}ï¼Œå›æ‡‰é•·åº¦: {len(content)}")
        return content
    except requests.exceptions.Timeout:
        logging.error(f"OpenAI API request timed out. Model: {model}")
        return f"æŠ±æ­‰ï¼Œè«‹æ±‚ OpenAI ({model}) æœå‹™è¶…æ™‚ã€‚"
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI API request error: {e}. Model: {model}")
        return f"æŠ±æ­‰ï¼Œé€£æ¥ OpenAI ({model}) æœå‹™æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    except (KeyError, IndexError, TypeError) as e:
        response_text = response.text if 'response' in locals() else 'N/A'
        logging.error(f"OpenAI API response format error: {e} - Response: {response_text}")
        return f"æŠ±æ­‰ï¼ŒOpenAI ({model}) å›æ‡‰æ ¼å¼æœ‰å•é¡Œã€‚"
    except Exception as e:
        logging.error(f"Unexpected error in call_openai_api: {e}", exc_info=True)
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›è¦†æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ã€‚"

def generate_chat_response(prompt_text):
    system_prompt = (
        "ä½ æ˜¯ä¸€å€‹é€šè¨Šè»Ÿé«”çš„èŠå¤©æ©Ÿå™¨äººã€‚å›ç­”è¦ç²¾ç°¡ã€å£èªåŒ–ï¼Œä½¿ç”¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡ã€‚"
        "å¦‚æœç­”æ¡ˆéœ€è¦æ€è€ƒæ­¥é©Ÿï¼Œè«‹å°‡æ€è€ƒéç¨‹ç”¨ <think> å’Œ </think> æ¨™ç±¤åŒ…èµ·ä¾†ã€‚"
    )
    return call_openai_api(system_prompt, prompt_text)


# ==============================================================================
# --- æ–°èæ‘˜è¦èˆ‡æ•´åˆæ¨¡çµ„ (å…©éšæ®µæ‘˜è¦) ---
# ==============================================================================

def summarize_news_flow(articles_data):
    if not articles_data:
        logging.info("æ²’æœ‰æ–‡ç« å¯ä¾›æ‘˜è¦ã€‚")
        return "ä»Šå¤©æ²’æœ‰æŠ“å–åˆ°ç›¸é—œæ–°èå¯ä¾›æ‘˜è¦ã€‚"

    # --- Stage 1: Individual Summaries ---
    logging.info("--- é–‹å§‹ç¬¬ä¸€éšæ®µæ‘˜è¦ï¼šé€ç¯‡ç²¾ç°¡ ---")
    individual_summaries = []
    for i, article in enumerate(articles_data):
        logging.info(f"  æ­£åœ¨æ‘˜è¦ç¬¬ {i+1}/{len(articles_data)} ç¯‡: {article['title']}")
        content_to_summarize = article['text'][:8000]
        user_prompt = f"æ–°èæ¨™é¡Œï¼š{article['title']}\n\næ–°èå…§æ–‡ï¼š\n{content_to_summarize}"
        
        raw_summary = call_openai_api(
            system_prompt=PROMPT_FOR_INDIVIDUAL_SUMMARY, user_prompt=user_prompt,
            model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o-mini"),
            max_tokens=4500, temperature=0.2
        )
        
        if raw_summary.startswith("æŠ±æ­‰ï¼Œ"):
            logging.warning(f"  [è·³é] ç¬¬ {i+1} ç¯‡æ–°èæ‘˜è¦å¤±æ•—: {raw_summary}")
            continue

        # *** ä¿®æ”¹é–‹å§‹ ***
        # ä½¿ç”¨æ­£è¦è¡¨ç¤ºå¼ä¾†ç§»é™¤ <think>...</think> å€å¡Š
        # re.DOTALL è®“ '.' å¯ä»¥åŒ¹é…æ›è¡Œç¬¦ï¼Œre.IGNORECASE å¿½ç•¥å¤§å°å¯«
        think_pattern = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)
        cleaned_summary = re.sub(think_pattern, '', raw_summary).strip()
        
        # å¢åŠ æ—¥èªŒï¼Œæ–¹ä¾¿è§€å¯Ÿæ¸…ç†æ•ˆæœ
        if len(raw_summary) != len(cleaned_summary):
             logging.info(f"  å·²æ¸…ç†æ‰ <think> æ¨™ç±¤ã€‚åŸå§‹é•·åº¦: {len(raw_summary)}, æ¸…ç†å¾Œé•·åº¦: {len(cleaned_summary)}")
        # *** ä¿®æ”¹çµæŸ ***

        individual_summaries.append({'title': article['title'], 'url': article['url'], 'summary': cleaned_summary})
        logging.info(f"  æ‘˜è¦å®Œæˆï¼Œé•·åº¦: {len(cleaned_summary)} å­—")
        print(individual_summaries[-1])
        time.sleep(1)

    if not individual_summaries:
        logging.warning("æ‰€æœ‰æ–°èåœ¨ç¬¬ä¸€éšæ®µæ‘˜è¦éƒ½å¤±æ•—äº†ã€‚")
        return "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆéç¨‹ç™¼ç”Ÿå•é¡Œï¼Œç„¡æ³•ç”¢å‡ºå…§å®¹ã€‚"

    # --- Stage 2: Final Aggregation ---
    logging.info("--- é–‹å§‹ç¬¬äºŒéšæ®µæ‘˜è¦ï¼šå½™æ•´ç”Ÿæˆ Podcast å…§å®¹ ---")
    summaries_for_prompt = [f"æ–°è {i+1}:\næ¨™é¡Œ: {item['title']}\næ‘˜è¦å…§å®¹: {item['summary']}\n---" for i, item in enumerate(individual_summaries)]
    final_user_prompt = "\n".join(summaries_for_prompt)
    
    final_summary = call_openai_api(
        system_prompt=PROMPT_FOR_FINAL_AGGREGATION, user_prompt=final_user_prompt,
        model=os.getenv("OPENAI_COMPLETION_MODEL", "gpt-4o"),
        max_tokens=3000, temperature=0.7
    )
    return final_summary


# ==============================================================================
# --- Line Bot åŸºç¤åŠŸèƒ½ ---
# ==============================================================================

def load_user_preferences():
    try:
        with open(USER_PREFERENCES_FILE, "r", encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_user_preferences(preferences_data):
    try:
        with open(USER_PREFERENCES_FILE, "w", encoding='utf-8') as f:
            json.dump(preferences_data, f, ensure_ascii=False, indent=4)
        logging.info(f"ç”¨æˆ¶åå¥½å·²å„²å­˜ã€‚å…± {len(preferences_data)} ç­†è¨˜éŒ„ã€‚")
    except Exception as e:
        logging.error(f"å„²å­˜ç”¨æˆ¶åå¥½è¨­å®šå¤±æ•—: {e}")

USER_PREFERENCES = load_user_preferences()

def validate_signature(request_body_bytes, signature_header):
    if not LINE_CHANNEL_SECRET: return True
    hash_obj = hmac.new(LINE_CHANNEL_SECRET.encode('utf-8'), request_body_bytes, hashlib.sha256)
    generated_signature = base64.b64encode(hash_obj.digest()).decode('utf-8')
    return hmac.compare_digest(generated_signature, signature_header)

def split_long_message(text, limit=MAX_MESSAGE_LENGTH):
    messages = []
    if not text or not text.strip(): return []
    if len(text) <= limit: return [text.strip()]
    
    chunks = []
    current_chunk = ""
    paragraphs = text.split('\n')
    for para in paragraphs:
        if len(current_chunk) + len(para) + 1 <= limit:
            current_chunk += para + '\n'
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            # å¦‚æœå–®ä¸€æ®µè½å°±è¶…éé•·åº¦ï¼Œå¼·åˆ¶åˆ†å‰²
            if len(para) > limit:
                for i in range(0, len(para), limit):
                    chunks.append(para[i:i+limit])
            else:
                current_chunk = para + '\n'
    if current_chunk:
        chunks.append(current_chunk.strip())

    if len(chunks) > 1:
        for i, chunk in enumerate(chunks):
            page_indicator = f"({i+1}/{len(chunks)})\n"
            messages.append(page_indicator + chunk)
    else:
        messages = chunks
        
    return [m for m in messages if m]

def send_line_messages(user_id, reply_token_or_none, text_messages_list):
    if not text_messages_list:
        logging.warning(f"æ²’æœ‰è¨Šæ¯å¯ç™¼é€çµ¦ {user_id}")
        return

    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}", "Content-Type": "application/json"}
    
    # è™•ç†ç¬¬ä¸€å‰‡è¨Šæ¯ï¼Œå„ªå…ˆä½¿ç”¨ reply token
    first_message_sent = False
    if reply_token_or_none:
        payload = {"replyToken": reply_token_or_none, "messages": [{"type": "text", "text": str(text_messages_list[0])}]}
        try:
            response = requests.post("https://api.line.me/v2/bot/message/reply", headers=headers, json=payload, timeout=20)
            response.raise_for_status()
            logging.info(f"Reply message sent to user {user_id}. Content: {str(text_messages_list[0])[:50]}...")
            first_message_sent = True
        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to send Reply message to {user_id}: {e} - Response: {e.response.text if e.response else 'N/A'}")
    
    # å¾ŒçºŒè¨Šæ¯æˆ–æ²’æœ‰ reply token çš„æƒ…æ³ï¼Œä½¿ç”¨ push API
    start_index = 1 if first_message_sent else 0
    for i in range(start_index, len(text_messages_list)):
        if not user_id:
            logging.error("Cannot push message: user_id is missing.")
            continue
        
        # åŠ å…¥å»¶é²é¿å…ç™¼é€éå¿«
        if i > 0 or first_message_sent:
            time.sleep(VISUAL_SEPARATION_DELAY)
            
        payload = {"to": user_id, "messages": [{"type": "text", "text": str(text_messages_list[i])}]}
        try:
            response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
            response.raise_for_status()
            logging.info(f"Push message sent to user {user_id} (Part {i+1}/{len(text_messages_list)}). Content: {str(text_messages_list[i])[:50]}...")
        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to send Push message to {user_id}: {e} - Response: {e.response.text if e.response else 'N/A'}")


# ==============================================================================
# --- æ ¸å¿ƒæ¥­å‹™é‚è¼¯èˆ‡ Webhook äº‹ä»¶è™•ç† ---
# ==============================================================================

def generate_and_push_news_for_user(user_id, user_custom_keywords=None, is_immediate_push=False, test_limit=None, reply_token=None):
    """ç‚ºæŒ‡å®šç”¨æˆ¶ç²å–ã€æ‘˜è¦ä¸¦æ¨æ’­æ–°èçš„å®Œæ•´æµç¨‹"""
    log_prefix = "å³æ™‚è«‹æ±‚" if is_immediate_push else "æ’ç¨‹æ¨æ’­"
    logging.info(f"[{log_prefix}] é–‹å§‹ç‚ºç”¨æˆ¶ {user_id} ç”¢ç”Ÿæ–°è...")
    
    # æ­¥é©Ÿ 1: æŠ“å–æ–‡ç« 
    articles = fetch_and_parse_articles(
        custom_query=user_custom_keywords, 
        limit=test_limit if test_limit is not None else NEWS_FETCH_TARGET_COUNT
    )

    if not articles:
        keywords_msg = f"ã€Œ{user_custom_keywords}ã€" if user_custom_keywords else "é è¨­ä¸»é¡Œ"
        message_to_send = f"æŠ±æ­‰ï¼Œç›®å‰æœªèƒ½æ ¹æ“šæ‚¨çš„é—œéµå­— {keywords_msg} æ‰¾åˆ°å¯æˆåŠŸæ“·å–çš„æ–°èã€‚è¦ä¸è¦æ›å€‹é—œéµå­—è©¦è©¦çœ‹ï¼Ÿ"
        send_line_messages(user_id, reply_token, [message_to_send])
        logging.info(f"[{log_prefix}] æ²’æœ‰æŠ“åˆ°æ–‡ç« ï¼Œå·²é€šçŸ¥ç”¨æˆ¶ {user_id}ã€‚")
        return
    
    # æ­¥é©Ÿ 2: åŸ·è¡Œå…©éšæ®µæ‘˜è¦ï¼Œç²å–æœ€çµ‚å¯èƒ½åŒ…å« <think> çš„å­—ä¸²
    final_summary_with_think = summarize_news_flow(articles)
    
    # *** ä¿®æ”¹é–‹å§‹ ***
    # æ­¥é©Ÿ 3: ä½¿ç”¨æ–°çš„å…±ç”¨å‡½å¼ä¾†è™•ç†ä¸¦ç™¼é€æœ€çµ‚æ‘˜è¦
    if not final_summary_with_think or final_summary_with_think.startswith("æŠ±æ­‰ï¼Œ"):
        logging.error(f"[{log_prefix}] æœ€çµ‚æ‘˜è¦ç‚ºç©ºæˆ–ç”Ÿæˆå¤±æ•—ï¼Œç„¡æ³•ç™¼é€çµ¦ {user_id}ã€‚")
        # å¦‚æœæ˜¯å³æ™‚è«‹æ±‚ï¼Œç”¨ reply_token å›è¦†ï¼›å¦å‰‡ç”¨ push
        send_line_messages(user_id, reply_token, [final_summary_with_think or "æŠ±æ­‰ï¼Œä»Šæ—¥æ–°èæ‘˜è¦ç”Ÿæˆç•°å¸¸ï¼Œå…§å®¹ç‚ºç©ºã€‚"])
        return

    # å°‡æœ€çµ‚æ‘˜è¦äº¤çµ¦æ–°çš„è™•ç†å‡½å¼ï¼Œå®ƒæœƒè‡ªå‹•è™•ç† <think> æ¨™ç±¤ä¸¦ç™¼é€
    handle_llm_response_with_think(user_id, reply_token, final_summary_with_think)
    # *** ä¿®æ”¹çµæŸ ***

    logging.info(f"[{log_prefix}] å·²å®Œæˆå°ç”¨æˆ¶ {user_id} çš„æ–°èæ¨é€ã€‚")

@app.route('/webhook', methods=['POST'])
def webhook():
    signature = request.headers.get("X-Line-Signature")
    body_bytes = request.get_data()
    if not validate_signature(body_bytes, signature):
        logging.error("Webhook: Invalid signature.")
        return jsonify({"status": "invalid signature"}), 400
    
    try:
        data = request.json
        for event in data.get("events", []):
            event_type = event.get("type")
            source = event.get("source", {})
            user_id = source.get("userId")
            reply_token = event.get("replyToken")
            
            logging.info(f"æ”¶åˆ°äº‹ä»¶: type={event_type}, user_id={user_id}")

            if event_type == "message" and event.get("message", {}).get("type") == "text":
                handle_text_message_event(user_id, reply_token, event["message"]["text"])
            
            elif event_type == "follow":
                if user_id and reply_token:
                    user_pref = USER_PREFERENCES.get(user_id, {})
                    user_pref["subscribed_news"] = True
                    user_pref["news_keywords"] = None # é è¨­ä½¿ç”¨å…¨å±€é—œéµå­—
                    USER_PREFERENCES[user_id] = user_pref
                    save_user_preferences(USER_PREFERENCES)
                    send_line_messages(user_id, reply_token, ["æ„Ÿè¬æ‚¨åŠ æˆ‘å¥½å‹ï¼æˆ‘å°‡å˜—è©¦æ¯æ—¥ç‚ºæ‚¨æ¨æ’­AIç›¸é—œæ–°èå½™æ•´ã€‚è¼¸å…¥ã€Œè¨‚é–±æ–°è [æ‚¨çš„é—œéµå­—]ã€å¯è‡ªè¨‚ä¸»é¡Œï¼Œæˆ–è¼¸å…¥ã€Œå–æ¶ˆè¨‚é–±æ–°èã€ä¾†å–æ¶ˆæ¨æ’­ã€‚"])

            elif event_type == "unfollow":
                if user_id in USER_PREFERENCES:
                    logging.info(f"ç”¨æˆ¶ {user_id} å·²å°é–/åˆªé™¤å¥½å‹ã€‚")
                    USER_PREFERENCES[user_id]["subscribed_news"] = False
                    save_user_preferences(USER_PREFERENCES)

        return jsonify({"status": "success"}), 200
    except Exception as e:
        logging.error(f"è™•ç† webhook æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500

def handle_llm_response_with_think(user_id, reply_token, llm_full_response):
    """
    è™•ç†å¸¶æœ‰ <think> æ¨™ç±¤çš„ LLM å›æ‡‰ï¼Œä¸¦å°‡å…¶åˆ†é›¢ç™¼é€ã€‚
    é€™æ˜¯ä¸€å€‹å¯å…±ç”¨çš„å‡½å¼ï¼Œç”¨æ–¼èŠå¤©å’Œæ–°èæ‘˜è¦ã€‚
    """
    think_pattern = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
    match = think_pattern.search(llm_full_response)
    
    reply_token_has_been_used = False

    if match:
        thinking_process_text = match.group(1).strip()
        formal_reply_text = llm_full_response[match.end():].strip()
        logging.info(f"CoT found for {user_id}. Thinking: '{thinking_process_text[:30]}...', Formal: '{formal_reply_text[:30]}...'")
        
        # ç™¼é€æ€è€ƒéç¨‹
        if thinking_process_text:
            think_chunks = split_long_message(f"âš™ï¸ æˆ‘çš„æ€è€ƒéç¨‹ï¼š\n{thinking_process_text}")
            if think_chunks:
                send_line_messages(user_id, reply_token, think_chunks)
                reply_token_has_been_used = True
        
        # å¦‚æœæœ‰æ€è€ƒéç¨‹ï¼Œç­‰å¾…ä¸€ä¸‹å†ç™¼é€æ­£å¼å›è¦†
        if reply_token_has_been_used and formal_reply_text:
            logging.info(f"Delaying {VISUAL_SEPARATION_DELAY}s before sending formal reply to {user_id}.")
            time.sleep(VISUAL_SEPARATION_DELAY)
        
        # ç™¼é€æ­£å¼å›è¦†
        if formal_reply_text:
            formal_chunks = split_long_message(formal_reply_text)
            # å¦‚æœ reply_token å·²ç”¨éï¼Œé€™è£¡å¿…é ˆå‚³å…¥ None
            send_line_messages(user_id, None if reply_token_has_been_used else reply_token, formal_chunks)
        elif not thinking_process_text: # åªæœ‰ <think></think> æ¨™ç±¤ä½†å…§å®¹ç‚ºç©º
            send_line_messages(user_id, reply_token, ["å—¯...æˆ‘å¥½åƒä»€éº¼éƒ½æ²’æƒ³åˆ°ã€‚"])

    else:
        # æ²’æœ‰ <think> æ¨™ç±¤ï¼Œç›´æ¥ç™¼é€å®Œæ•´å›æ‡‰
        logging.info(f"No CoT found in LLM response for {user_id}.")
        response_chunks = split_long_message(llm_full_response)
        if not response_chunks and llm_full_response.strip():
            response_chunks = [llm_full_response.strip()]
        elif not response_chunks:
            response_chunks = ["æˆ‘ç›®å‰æ²’æœ‰å›æ‡‰ã€‚"]
        send_line_messages(user_id, reply_token, response_chunks)
        
def handle_text_message_event(user_id, reply_token, user_text):
    user_text_stripped = user_text.strip()
    user_text_lower = user_text_stripped.lower()
    
    subscribe_command = "è¨‚é–±æ–°è"
    unsubscribe_command = "å–æ¶ˆè¨‚é–±æ–°è"
    user_pref = USER_PREFERENCES.get(user_id, {})

    if user_text_stripped.startswith(subscribe_command):
        keywords_from_user = user_text_stripped[len(subscribe_command):].strip()
        user_pref["subscribed_news"] = True
        
        if keywords_from_user:
            user_pref["news_keywords"] = keywords_from_user
            reply_msg = f"å¥½çš„ğŸ‘Œï¼å·²ç‚ºæ‚¨è¨‚é–±æ¯æ—¥æ–°èï¼Œé—œéµå­—ç‚ºï¼šã€Œ{keywords_from_user}ã€ã€‚æˆ‘é¦¬ä¸Šç‚ºæ‚¨æ•´ç†ä¸€ä»½æœ€æ–°çš„ï¼"
        else:
            user_pref["news_keywords"] = None
            reply_msg = f"å¥½çš„ğŸ‘Œï¼å·²ç‚ºæ‚¨è¨‚é–±æ¯æ—¥æ–°èï¼ˆä½¿ç”¨é è¨­ä¸»é¡Œï¼‰ã€‚æˆ‘é¦¬ä¸Šç‚ºæ‚¨æ•´ç†ä¸€ä»½æœ€æ–°çš„ï¼"
        
        USER_PREFERENCES[user_id] = user_pref
        save_user_preferences(USER_PREFERENCES)
        
        # ç«‹å³è§¸ç™¼æ–°èæ¨æ’­ï¼Œé€™è£¡å‚³å…¥ reply_token
        # æ–°çš„ generate_and_push_news_for_user å‡½å¼æœƒæ­£ç¢ºè™•ç†å®ƒ
        generate_and_push_news_for_user(user_id, user_pref["news_keywords"], is_immediate_push=True, reply_token=reply_token)
        return

    elif user_text_lower == unsubscribe_command:
        user_pref["subscribed_news"] = False
        USER_PREFERENCES[user_id] = user_pref
        save_user_preferences(USER_PREFERENCES)
        send_line_messages(user_id, reply_token, ["å·²ç‚ºæ‚¨å–æ¶ˆè¨‚é–±æ¯æ—¥æ–°èã€‚æ±Ÿæ¹–å†è¦‹ï¼"])
        return

    # --- ä¸€èˆ¬å°è©±æ©Ÿå™¨äººé‚è¼¯ ---
    is_triggered = any(user_text_lower.startswith(name.lower()) for name in BOT_NAMES if name)
    if BOT_DEACTIVATED or not is_triggered:
        return

    prompt_for_llm = user_text_stripped
    for name in BOT_NAMES:
        if name and user_text_lower.startswith(name.lower()):
            prompt_for_llm = user_text_stripped[len(name):].strip()
            break
    
    if not prompt_for_llm:
        send_line_messages(user_id, reply_token, ["å—¨ï¼æœ‰ä»€éº¼äº‹å—ï¼Ÿ"])
        return

    llm_response = generate_chat_response(prompt_for_llm)

    # *** ä¿®æ”¹é–‹å§‹ ***
    # ç›´æ¥å‘¼å«æ–°çš„å…±ç”¨å‡½å¼ä¾†è™•ç†å›è¦†
    handle_llm_response_with_think(user_id, reply_token, llm_response)
    # *** ä¿®æ”¹çµæŸ ***


# ==============================================================================
# --- æ’ç¨‹èˆ‡æ‡‰ç”¨å•Ÿå‹• ---
# ==============================================================================
scheduler = BackgroundScheduler(timezone="Asia/Taipei", daemon=True)

def daily_news_push_job():
    with app.app_context():
        logging.info("APScheduler: é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæ¨æ’­ä»»å‹™...")
        current_prefs = load_user_preferences()
        
        users_to_push = []
        for uid, prefs in current_prefs.items():
            if prefs.get("subscribed_news", False):
                users_to_push.append((uid, prefs.get("news_keywords")))

        if TARGET_USER_ID_FOR_TESTING:
            is_test_user_subscribed = any(u[0] == TARGET_USER_ID_FOR_TESTING for u in users_to_push)
            if not is_test_user_subscribed:
                test_user_keywords = current_prefs.get(TARGET_USER_ID_FOR_TESTING, {}).get("news_keywords")
                users_to_push.append((TARGET_USER_ID_FOR_TESTING, test_user_keywords))
                logging.info(f"APScheduler: å°‡æ¸¬è©¦ç”¨æˆ¶ {TARGET_USER_ID_FOR_TESTING} åŠ å…¥æ¨æ’­åˆ—è¡¨ã€‚")

        if not users_to_push:
            logging.info("APScheduler: æ²’æœ‰éœ€è¦æ¨æ’­çš„ç”¨æˆ¶ã€‚")
            return
            
        logging.info(f"APScheduler: æº–å‚™æ¨æ’­æ–°èçµ¦ {len(users_to_push)} ä½ç”¨æˆ¶ã€‚")

        for user_id, keywords in users_to_push:
            try:
                generate_and_push_news_for_user(user_id, keywords, is_immediate_push=False)
                time.sleep(5) # é¿å…å° API å’Œç¶²ç«™é€ æˆå¤ªå¤§å£“åŠ›
            except Exception as e:
                logging.error(f"APScheduler: ç‚ºç”¨æˆ¶ {user_id} æ¨æ’­æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        
        logging.info("APScheduler: æ¯æ—¥æ–°èæ¨æ’­ä»»å‹™åŸ·è¡Œå®Œç•¢ã€‚")

def shutdown_scheduler_on_exit():
    if scheduler.running:
        scheduler.shutdown(wait=False)
        logging.info("APScheduler shut down.")


# ==============================================================================
# --- âœ¨ æœ¬åœ°æ¸¬è©¦æ¨¡å¼ (ä¿®æ”¹å¾Œç‰ˆæœ¬) âœ¨ ---
# ==============================================================================
def run_test_mode(keywords, limit):
    """åŸ·è¡Œæœ¬åœ°æ¸¬è©¦æµç¨‹ï¼Œä¸¦æ¨¡æ“¬æœ€çµ‚ç™¼é€è¡Œç‚º"""
    print("\n" + "="*50)
    print("ğŸš€ é€²å…¥æœ¬åœ°æ¸¬è©¦æ¨¡å¼ ğŸš€")
    print("="*50 + "\n")
    
    test_keywords = keywords if keywords else None
    test_limit = limit if limit is not None else NEWS_FETCH_TARGET_COUNT

    print(f"[*] æ¸¬è©¦åƒæ•¸:")
    print(f"    - é—œéµå­—: '{test_keywords if test_keywords else 'é è¨­é—œéµå­—'}'")
    print(f"    - è™•ç†æ–‡ç« ä¸Šé™: {test_limit}\n")

    articles = fetch_and_parse_articles(custom_query=test_keywords, limit=test_limit)
    
    if not articles:
        print("\n[!] æ¸¬è©¦ä¸­æ­¢ï¼šæœªèƒ½æˆåŠŸæ“·å–ä»»ä½•æ–°èå…§æ–‡ã€‚")
        return
        
    final_summary_with_think = summarize_news_flow(articles)
    
    # *** ä¿®æ”¹é–‹å§‹ ***
    # æˆ‘å€‘ä¸å†ç›´æ¥ print åŸå§‹æ‘˜è¦ï¼Œè€Œæ˜¯æ¨¡æ“¬ handle_llm_response_with_think çš„è¡Œç‚º
    
    print("\n" + "="*50)
    print("ğŸ“¦ æ¨¡æ“¬è¨Šæ¯ç™¼é€ (é è¦½ç™¼é€çµ¦ Line çš„æœ€çµ‚å…§å®¹)")
    print("="*50)

    think_pattern = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
    match = think_pattern.search(final_summary_with_think)
    
    if match:
        thinking_process_text = match.group(1).strip()
        formal_reply_text = final_summary_with_think[match.end():].strip()
        
        # æ¨¡æ“¬ç™¼é€æ€è€ƒéç¨‹
        if thinking_process_text:
            think_chunks = split_long_message(f"âš™ï¸ æˆ‘çš„æ€è€ƒéç¨‹ï¼š\n{thinking_process_text}")
            print(f"--- åµæ¸¬åˆ°æ€è€ƒéç¨‹ (å…± {len(think_chunks)} å‰‡è¨Šæ¯) ---")
            for i, chunk in enumerate(think_chunks):
                print(f"--- [æ€è€ƒè¨Šæ¯ {i+1}] ---\n{chunk}")
            print("-" * 20)

        # æ¨¡æ“¬ç™¼é€æ­£å¼å›è¦†
        if formal_reply_text:
            formal_chunks = split_long_message(formal_reply_text)
            print(f"--- æ­£å¼å›è¦† (å…± {len(formal_chunks)} å‰‡è¨Šæ¯) ---")
            for i, chunk in enumerate(formal_chunks):
                print(f"--- [æ­£å¼è¨Šæ¯ {i+1}] ---\n{chunk}")
        
    else:
        # æ²’æœ‰ <think> æ¨™ç±¤ï¼Œç›´æ¥æ¨¡æ“¬ç™¼é€å®Œæ•´å›æ‡‰
        print("--- æœªåµæ¸¬åˆ°æ€è€ƒéç¨‹ï¼Œç›´æ¥ç™¼é€ ---")
        response_chunks = split_long_message(final_summary_with_think)
        for i, chunk in enumerate(response_chunks):
            print(f"--- [è¨Šæ¯ {i+1}] ---\n{chunk}")
            
    # *** ä¿®æ”¹çµæŸ ***

    print("\n" + "="*50)
    print("âœ… æ¸¬è©¦æµç¨‹çµæŸ âœ…")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Line Bot and News Fetcher")
    parser.add_argument('--test-news', action='store_true', help='Run in local test mode for news fetching and summarization.')
    parser.add_argument('--keywords', type=str, default=None, help='Keywords for news fetching in test mode.')
    parser.add_argument('--limit', type=int, default=None, help='Number of articles to process in test mode.')
    args = parser.parse_args()

    if args.test_news:
        run_test_mode(args.keywords, args.limit)
    else:
        logging.info("ğŸš€ å•Ÿå‹• Flask Web ä¼ºæœå™¨æ¨¡å¼ ğŸš€")
        
        required_env_vars = ['LINE_CHANNEL_ACCESS_TOKEN', 'LINE_CHANNEL_SECRET', 'OPENAI_API_KEY']
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        if missing_vars:
            logging.critical(f"CRITICAL: Missing required environment variables: {', '.join(missing_vars)}. Exiting.")
            exit(1)

        if not scheduler.get_jobs():
            # è¨­å®šæ’ç¨‹ä»»å‹™
            # 1. æ¯æ—¥ 09:00 çš„å›ºå®šæ’ç¨‹
            scheduler.add_job(daily_news_push_job, 'cron', hour=9, minute=0, id='daily_news_cron', replace_existing=True)
            logging.info("å·²è¨­å®šæ¯æ—¥ 09:00 çš„æ–°èæ¨æ’­æ’ç¨‹ã€‚")
            
            # 2. æ¯ 480 åˆ†é˜ (8 å°æ™‚) çš„é–“éš”æ’ç¨‹
            scheduler.add_job(daily_news_push_job, 'interval', minutes=480, id='news_interval_job', replace_existing=True)
            logging.info("å·²è¨­å®šæ¯ 480 åˆ†é˜åŸ·è¡Œä¸€æ¬¡æ–°èæ¨æ’­æ’ç¨‹ã€‚")
            
            # 3. ç¨‹å¼å•Ÿå‹•æ™‚ç«‹å³åŸ·è¡Œä¸€æ¬¡çš„é¸é …
            if os.getenv("RUN_JOB_ON_STARTUP", "False").lower() == "true":
                run_now_time = datetime.now(scheduler.timezone) + timedelta(seconds=15)
                scheduler.add_job(daily_news_push_job, 'date', run_date=run_now_time, id='startup_news_push')
                logging.info(f"å·²è¨­å®šåœ¨ 15 ç§’å¾ŒåŸ·è¡Œä¸€æ¬¡æ–°èæ¨æ’­ä»»å‹™ã€‚")
        
        if not scheduler.running:
            scheduler.start()
            logging.info("APScheduler started.")
            atexit.register(shutdown_scheduler_on_exit)
        
        port = int(os.environ.get("PORT", 5000))
        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)